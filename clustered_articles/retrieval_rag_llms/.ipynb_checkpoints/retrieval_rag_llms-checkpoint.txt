Filename	Abstract
s12652-019-01247-9.pdf	Nowadays, searching the relevant documents from a large dataset becomes a big challenge. Automatic query expansion is one of the techniques, which addresses this problem by refining the query. A new query expansion approach using cuckoo search and accelerated particle swarm optimization technique is proposed in this paper. The proposed approach mainly focused to find the most relevant expanded query rather than suitable expansion terms. In this paper, Fuzzy logic is also employed, which improves the performance of accelerated particle swarm optimization by controlling various parameters. We have compared the proposed approach with other existing and recently developed automatic query expansion approaches on various evaluating parameters such as average recall, average precision, Mean-Average Precision, F-measure and precision- recall graph. We have evaluated the performance of all approaches on three datasets CISI, CACM and TREC-3. The results obtained for all three datasets depict that the proposed approach gets better results in comparison to other automatic query expansion approaches. Keywords Document retrieval · Accelerated particle swarm optimization · Cuckoo search · Fuzzy logic · Precision · Recall and F-measure

2408.11875v1.pdf	Multi-hop Question Answering (QA) necessitates complex reason- ing by integrating multiple pieces of information to resolve intri- cate questions. However, existing QA systems encounter challenges such as outdated information, context window length limitations, and an accuracy-quantity trade-off. To address these issues, we propose a novel framework, the Hierarchical Retrieval-Augmented Generation Model with Rethink (HiRAG), comprising Decomposer, Definer, Retriever, Filter, and Summarizer five key modules. We introduce a new hierarchical retrieval strategy that incorporates both sparse retrieval at the document level and dense retrieval at the chunk level, effectively integrating their strengths. Addi- tionally, we propose a single-candidate retrieval method to miti- gate the limitations of multi-candidate retrieval. We also construct two new corpora, Indexed Wikicorpus and Profile Wikicorpus, to address the issues of outdated and insufficient knowledge. Our experimental results on four datasets demonstrate that HiRAG outperforms state-of-the-art models across most metrics, and our Indexed Wikicorpus is effective. The code for HiRAG is available at CCS Concepts • Information systems →Combination, fusion and federated search. Keywords Large Language Models, Hierarchical Retrieval-Augmented Gener- ation, Indexed Wikicorpus Corpus, Profile Corpus ∗These authors contributed equally to this research. †Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Conference acronym, 2024, Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM ACM Reference Format: Xiaoming Zhang, Ming Wang, Xiaocui Yang, Daling Wang, Shi Feng, and Yifei Zhang. 2018. Hierarchical Retrieval-Augmented Generation Model with Re- think for Multi-hop Question Answering. In Proceedings of (Conference acronym). ACM, New York, NY, USA, 12 pages. XXXXXXX

978-3-031-30399-9_10.pdf	Community discovery is one of the most studied problems in network science. In recent years, many works have focused on discovering communities in temporal networks, thus identifying dynamic communities. Interestingly, dynamic communities are not mere sequences of static ones; new challenges arise from their dynamic nature. Despite the large number of algorithms introduced in the literature, some of these challenges have been overlooked or little studied until recently. In this chapter, we will discuss some of these challenges and recent propositions to tackle them.Wewill,amongothertopics,discussofcommunityeventsingraduallyevolving networks, on the notion of identity through change and the ship of Theseus paradox, on dynamic communities in different types of networks including link streams, on the smoothness of dynamic communities, and on the different types of complexity of algorithms for their discovery. We will also list available tools and libraries adapted to work with this problem. Keywords Temporal networks · Community detection 10.1

2409.01666v1.pdf	Overcoming the limited context limitations in early-generation LLMs, retrieval-augmented generation (RAG) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context LLMs allows the models to incorporate much longer text sequences, making RAG less attractive. Recent studies show that long-context LLMs significantly outperform RAG in long-context applications. Unlike the existing works favor- ing the long-context LLM over RAG, we ar- gue that the extremely long context in LLMs suffers from a diminished focus on relevant in- formation and leads to potential degradation in answer quality. This paper revisits the RAG in long-context answer generation. We propose an order-preserve retrieval-augmented genera- tion (OP-RAG) mechanism, which significantly improves the performance of RAG for long- context question-answer applications. With OP-RAG, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where OP-RAG could achieve higher answer quality with much less tokens than long-context LLM taking the whole context as input. Extensive experiments on public benchmark demonstrate the superior- ity of our OP-RAG.

CIRCLE_2022_paper_27.pdf	Minghan Li1, Eric Gaussier1

3626772.3657834.pdf	Retrieval-Augmented Generation (RAG) has recently emerged as a method to extend beyond the pre-trained knowledge of Large Language Models by augmenting the original prompt with relevant passages or documents retrieved by an Information Retrieval (IR) system. RAG has become increasingly important for Generative AI solutions, especially in enterprise settings or in any domain in which knowledge is constantly refreshed and cannot be memorized in the LLM. We argue here that the retrieval component of RAG systems, be it dense or sparse, deserves increased attention from the research community, and accordingly, we conduct the first com- prehensive and systematic examination of the retrieval strategy of RAG systems. We focus, in particular, on the type of passages IR systems within a RAG solution should retrieve. Our analysis considers multiple factors, such as the relevance of the passages in- cluded in the prompt context, their position, and their number. One counter-intuitive finding of this work is that the retriever’s highest- scoring documents that are not directly relevant to the query (e.g., do not contain the answer) negatively impact the effectiveness of the LLM. Even more surprising, we discovered that adding random documents in the prompt improves the LLM accuracy by up to

3626772.3657984.pdf	In recent years, Retrieval Augmented Generation (RAG) systems have emerged as a pivotal component in the field of artificial intelli- gence, gaining significant attention and importance across various domains. These systems, which combine the strengths of infor- mation retrieval and generative models, have shown promise in enhancing the capabilities and performance of machine learning applications. However, despite their growing prominence, RAG sys- tems are not without their limitations and continue to be in need of exploration and improvement. This workshop seeks to focus on the critical aspect of information retrieval and its integral role within RAG frameworks. We argue that current efforts have undervalued the role of Information Retrieval (IR) in the RAG and have concen- trated their attention on the generative part. As the cornerstone of these systems, IR’s effectiveness dramatically influences the overall performance and outcomes of RAG models. We call for papers that will seek to revisit and emphasize the fundamental principles under- pinning RAG systems. At the end of the workshop, we aim to have a clearer understanding of how robust information retrieval mech- anisms can significantly enhance the capabilities of RAG systems. The workshop will serve as a platform for experts, researchers, and practitioners. We intend to foster discussions, share insights, and encourage research that underscores the vital role of Information Retrieval in the future of generative systems. CCS CONCEPTS • Information systems →Novelty in information retrieval. KEYWORDS Retrieval Augmented Generation, Generative Models, Neural Databases ACM Reference Format: Fabio Petroni, Federico Siciliano, Fabrizio Silvestri, and Giovanni Trap- polini. 2024. IR-RAG @ SIGIR24: Information Retrieval’s Role in RAG Systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’24), July

2410.18790v1.pdf	—Generative artificial intelligence (GAI) has emerged as a pivotal technology for content generation, reasoning, and decision-making, making it a promising solution on the 6G stage characterized by openness, connected intelligence, and service democratization. This article explores strategies for integrating and monetizing GAI within future open 6G networks, mainly from the perspectives of mobile network operators (MNOs). We propose a novel API-centric telecoms GAI marketplace platform, designed to serve as a central hub for deploying, managing, and monetizing diverse GAI services directly within the network. This platform underpins a flexible and interoperable ecosystem, enhances service delivery, and facilitates seamless integration of GAI capabilities across various network segments, thereby enabling new revenue streams through customer-centric generative services. Results from experimental evaluation in an end-to-end Open RAN testbed, show the latency benefits of this platform for local large language model (LLM) deployment, by comparing token timing for various generated lengths with cloud- based general-purpose LLMs. Lastly, the article discusses key considerations for implementing the GAI marketplace within 6G networks, including monetization strategy, regulatory, manage- ment, and service platform aspects. Index Terms—6G, generative AI, large language models, mar- ketplace, monetization, open networks, platform. I. INTRODUCTION Generative artificial intelligence (GAI) has emerged as a compelling and prominent research area due to its proven suc- cess in content generation services. Large GAI models, such as large language models (LLMs), image and video generation models, and multi-modality models, excel at understanding language and performing general-purpose tasks. Models like GPT-4, Gemini, LLaMA, and Claude demon- strate powerful capabilities in context understanding, planning, responding, and code generation. These models can be cus- tomized for specific industries using techniques like retrieval- augmented generation (RAG), low-rank adaptation (LoRA), and prompt-tuning for cost-effective updates. Integration of AI and communication networks is at the heart of 6G evolution, as highlighted in the “IMT-2030 Frame- work” . GAI, especially through LLMs , is seen as a key enabler of this integration, enhancing wireless communi- cation systems with advanced understanding, reasoning, and generating capabilities – essential for developing in-network intelligence. GAI is increasingly being adopted as a service in the telecoms sector. For example, LLMs are used for analyzing The authors are with the Bristol Research and Innovation Laboratory, Toshiba Europe Ltd., U.K. (This work was supported by the 6G-GOALS project under the 6G SNS-JU Horizon program, n.101139232.

280077.pdf	

2312.05934v3.pdf	Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their abil- ity to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen in- formation poses a significant challenge. In this study, we compare two common approaches: un- supervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while un- supervised fine-tuning offers some improvement, RAG consistently outperforms it, both for exist- ing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that expos- ing them to numerous variations of the same fact during training could alleviate this problem. and Factuality.

2403.10131v2.pdf	Pretraining Large Language Models (LLMs) on large corpora of textual data is now a standard paradigm. When using these LLMs for many downstream applications, it is common to additionally incorporate new in- formation into the pretrained model either through RAG-based-prompting, or finetuning. However, the best methodology to incorporate information remains an open question. In this paper, we present Retrieval Augmented Fine Tuning (RAFT), a training recipe which improves the model’s ability to answer questions in "open-book" in-domain settings. In training RAFT, given a question, and a set of retrieved documents, we train the model to ignore those documents that don’t help in answering the question, which we call, distractor documents. RAFT accomplishes this by citing verbatim the right sequence from the relevant document to help answer the question. This coupled with RAFT’s chain-of-thought-style response helps improve the model’s ability to reason. In domain specific RAG, RAFT consistently improves the model’s performance across PubMed, HotpotQA, and Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs to in-domain RAG.

Scientific_Documents_Retrieval_Based_on_Graph_Convolutional_Network_and_Hesitant_Fuzzy_Set.pdf	Previous scientific literature retrieval methods, which are based on mathematical expression, ignore the literature attributes and the association between the literature, and the retrieval accuracy was affected. In this study, literature retrieval model based on Graph Convolutional Network (GCN) is proposed. By extracting document attributes from a structured document dataset, an Attribute Relation Graph (ARG) is constructed. Using GCN to capture the dependencies among literature nodes and generate literature representations by information aggregation to realize graph-based literature modeling; Introducing the advantages of Hesitant Fuzzy Set (HFS) theory in multi-attribute decision-making to realize the similarity evaluation between mathematical query expressions and mathematical retrieval result expressions. Finally, the similarity between literature features and mathematical expressions is integrated to obtain the ordered output of scientific literature retrieval results. Experiments were conducted on the arXiv public dataset, and the average precision of the top 10 retrieval results was 0.892, and the average NDCG value of the top 10 rankings was 0.875. INDEX TERMS Scientific document retrieval, GCN (graph convolutional network), mathematical expres- sions, HFS (hesitant fuzzy set).

2403.14403v2.pdf	Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches deal- ing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the sim- ple or complex categories. In this work, we propose a novel adaptive QA framework that can dynamically select the most suitable strat- egy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selec- tion process is operationalized with a classi- fier, which is a smaller LM trained to predict the complexity level of incoming queries with automatically collected labels, obtained from actual predicted outcomes of models and in- herent inductive biases in datasets. This ap- proach offers a balanced strategy, seamlessly adapting between the iterative and single-step retrieval-augmented LLMs, as well as the no- retrieval methods, in response to a range of query complexities. We validate our model on a set of open-domain QA datasets, cov- ering multiple query complexities, and show that ours enhances the overall efficiency and accuracy of QA systems, compared to rele- vant baselines including the adaptive retrieval approaches. Code is available at: https:// github.com/starsuzi/Adaptive-RAG.

2307.11019v2.pdf	Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require a substantial amount of factual knowledge and often rely on external information for assistance. Recently, large language models (LLMs) (e.g., ChatGPT), have demonstrated impressive prowess in solv- ing a wide range of tasks with world knowledge, including knowledge-intensive tasks. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly how they behave when incorporat- ing retrieval augmentation. In this study, we present an initial analysis of the factual knowl- edge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain QA. Specially, we focus on three primary re- search questions and analyze them by examin- ing QA performance, priori judgement and pos- teriori judgement of LLMs. We show evidence that LLMs possess unwavering confidence in their capabilities to respond to questions and the accuracy of their responses. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs’ awareness of knowledge boundaries, thereby improving their judgemental abilities. Additionally, we also find that LLMs have a propensity to rely on the provided retrieval results when formulating an- swers, while the quality of these results signifi- cantly impacts their reliance. The code to repro- duce this work is available at com/RUCAIBox/LLM-Knowledge-Boundary.

2409.14924v1.pdf	Large language models (LLMs) augmented with external data have demonstrated remarkable capa- bilities in completing real-world tasks. External data not only bolsters the models’ domain-specific expertise and temporal relevance but also diminishes incidences of hallucination, thereby enhancing both the controllability and interpretability of outputs. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing atten- tion and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and the task’s primary focus: explicit fact queries, implicit fact queries, interpretable ratio- nale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.

2405.03122v2.pdf	—6G Open Radio Access Networks (O-RAN) promises to open data interfaces to enable plug-and-play service Apps, many of which are consumer and business-facing. Opening up

805-Article Text-3912-2-10-20240526.pdf	This study examines archive storage implementation to facilitate document retrieval at PT Kusuma Satria Dinasasri Wisatajaya Batu's Human Resources Department. It adopts a qualitative approach, employing observation, interviews, and documentation for data collection. The research shows that the way archives are stored isn't as good as it could be. This is demonstrated by problems like insufficient understanding and supervision of archives, no borrowing procedures, archives that keep growing, and a lack of staff that makes retrieval times longer. These challenges significantly impact job performance. Suggestions for improvement include enhancing employees' understanding of the importance of archives and organisational systems. We advise implementing a Record Retention Schedule (RRS) to manage archive accumulation effectively. Addressing space constraints and establishing borrowing procedures with defined periods are essential. Moreover, a monitoring system for archives is crucial to ensure proper organisation and maintenance. In conclusion, optimising archive storage practices is vital for efficient document retrieval and organisational effectiveness. By addressing identified issues and implementing suggested improvements, PT Kusuma Satria Dinasasri Wisatajaya Batu can enhance its archive management processes and support smoother operations in the Human Resources Department. JACE. This work is licensed under CC-BY 4.0

2403.19631v2.pdf	Large Language Models (LLMs) have shown proficiency in question- answering tasks but often struggle to integrate real-time knowl- edge, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi- hop questions, since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework for multi-hop question answering. RAE first re- trieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that traditional similarity-based searches might miss. In addition, our framework includes a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the editing accuracy and mitigates the hal- lucination problem. Our framework is supported by theoretical justification for its fact retrieval efficacy. Finally, comprehensive evaluation across various LLMs validates RAE’s ability in providing accurate answers with updated knowledge. Our code is available at: CCS CONCEPTS • Information systems →Question answering; • Computing

2405.06211v3.pdf	As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowl- edge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capac- ity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revo- lutionary abilities in language understanding and generation, while still facing inherent limitations, such as hallucinations and out-of- date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval- Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model’s internal knowledge, to augment the generation quality of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three pri- mary technical perspectives: architectures, training strategies, and applications. As the preliminary knowledge, we briefly introduce the foundations and recent advances of LLMs. Then, to illustrate the practical significance of RAG for LLMs, we systematically review mainstream relevant work by their architectures, training strate- gies, and application areas, detailing specifically the challenges of each and the corresponding capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss current limitations and sev- eral promising directions for future research. Updated information about this survey can be found at systems.github.io/RAG-Meets-LLMs/1. KEYWORDS Retrieval-Augmented Generation (RAG), Large Language Model (LLM), Pre-training, Fine-tuning, In-context Learning, Prompting. ∗Corresponding author: Yujuan Ding

2406.14891v2.pdf	Multi-Hop Question Answering (MHQA) tasks present a significant challenge for large language models (LLMs) due to the intensive knowledge required. Current solutions, like Retrieval-Augmented Generation, typically retrieve potential documents from an external corpus to read an answer. However, the performance of this retrieve-then-read paradigm is constrained by the retriever and the inevitable noise in the retrieved documents. To mitigate these challenges, we introduce a novel generate-then-ground (GenGround) framework, synergizing the parametric knowledge of LLMs and external documents to solve a multi-hop question. GenGround empowers LLMs to alternate two phases until the final answer is derived: formulate a simpler, single-hop question and directly generate the answer; ground the question-answer pair in retrieved documents, amending any wrong predictions in the answer. We also propose an instructional grounding distillation method to generalize our method into smaller models. Extensive experiments conducted on four datasets illustrate the superiority of our method.

2311.09210v2.pdf	Retrieval-augmented language model (RALM) represents a significant advancement in miti- gating factual hallucination by leveraging ex- ternal knowledge sources. However, the reli- ability of the retrieved information is not al- ways guaranteed, and the retrieval of irrele- vant data can mislead the response generation. Moreover, standard RALMs frequently neglect their intrinsic knowledge due to the interference from retrieved information. In instances where the retrieved information is irrelevant, RALMs should ideally utilize their intrinsic knowledge or, in the absence of both intrinsic and retrieved knowledge, opt to respond with "unknown" to avoid hallucination. In this paper, we intro- duces CHAIN-OF-NOTE (CON), a novel ap- proach to improve robustness of RALMs in facing noisy, irrelevant documents and in han- dling unknown scenarios. The core idea of CON is to generate sequential reading notes for each retrieved document, enabling a thor- ough evaluation of their relevance to the given question and integrating this information to for- mulate the final answer. Our experimental re- sults show that GPT-4, when equipped with CON, outperforms the CHAIN-OF-THOUGHT approach. Besides, we utilized GPT-4 to cre- ate 10K CON data, subsequently trained on LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that fine-tuned RALMs equipped with CON signifi- cantly outperform standard fine-tuned RALMs.

NeurIPS-2023-learning-to-tokenize-for-generative-retrieval-Paper-Conference.pdf	As a new paradigm in information retrieval, generative retrieval directly generates a ranked list of document identifiers (docids) for a given query using generative language models (LMs). How to assign each document a unique docid (denoted as document tokenization) is a critical problem, because it determines whether the generative retrieval model can precisely retrieve any document by simply decoding its docid. Most existing methods adopt rule-based tokenization, which is ad-hoc and does not generalize well. In contrast, in this paper we propose a novel document tokenization learning method, GENRET, which learns to encode the complete document semantics into docids. GENRET learns to tokenize documents into short discrete representations (i.e., docids) via a discrete auto-encoding approach. We develop a progressive training scheme to capture the autoregressive nature of docids and diverse clustering techniques to stabilize the training process. Based on the semantic-embedded docids of any set of documents, the generative retrieval model can learn to generate the most relevant docid only according to the docids’ semantic relevance to the queries. We conduct experiments on the NQ320K, MS MARCO, and BEIR datasets. GENRET establishes the new state-of-the-art on the NQ320K dataset. Compared to generative retrieval baselines, GENRET can achieve significant improvements on unseen documents. Moreover, GENRET can also outperform comparable baselines on MS MARCO and BEIR, demonstrating the method’s generalizability.

2408.08921v2.pdf	ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM 1557-735X/2024/9-ART111 J. ACM, Vol. 37, No. 4, Article 111. Publication date: September 2024. arXiv:2408.08921v2 [cs.AI] 10 Sep 2024

s10506-022-09341-8.pdf	Legal text retrieval serves as a key component in a wide range of legal text process- ing tasks such as legal question answering, legal case entailment, and statute law retrieval. The performance of legal text retrieval depends, to a large extent, on the representation of text, both query and legal documents. Based on good representa- tions, a legal text retrieval model can effectively match the query to its relevant docu- ments. Because legal documents often contain long articles and only some parts are relevant to queries, it is quite a challenge for existing models to represent such docu- ments. In this paper, we study the use of attentive neural network-based text repre- sentation for statute law document retrieval. We propose a general approach using deep neural networks with attention mechanisms. Based on it, we develop two hier- archical architectures with sparse attention to represent long sentences and articles, and we name them Attentive CNN and Paraformer. The methods are evaluated on datasets of different sizes and characteristics in English, Japanese, and Vietnamese. Experimental results show that: (i) Attentive neural methods substantially outperform non-neural methods in terms of retrieval performance across datasets and languages; (ii) Pretrained transformer-based models achieve better accuracy on small datasets at the cost of high computational complexity while lighter weight Attentive CNN achieves better accuracy on large datasets; and (iii) Our proposed Paraformer outper- forms state-of-the-art methods on COLIEE dataset, achieving the highest recall and F2 scores in the top-N retrieval task. Keywords Legal text retrieval · Deep neural networks · Hierarchical representation · Global attention This paper is an improved and extended work of Kien et al. . Ha-Thanh Nguyen and Manh-Kien Phi have contributed equally to this work. * Ha‑Thanh Nguyen nguyenhathanh@jaist.ac.jp Extended author information available on the last page of the article

Information_Retrieval_Recent_Advances_and_Beyond.pdf	This paper provides an extensive and thorough overview of the models and techniques utilized in the first and second stages of the typical information retrieval processing chain. Our discussion encompasses the current state-of-the-art models, covering a wide range of methods and approaches in the field of information retrieval. We delve into the historical development of these models, analyze the key advancements and breakthroughs, and address the challenges and limitations faced by researchers and practitioners in the domain. By offering a comprehensive understanding of the field, this survey is a valuable resource for researchers, practitioners, and newcomers to the information retrieval domain, fostering knowledge growth, innovation, and the development of novel ideas and techniques. INDEX TERMS First-stage retrieval, information retrieval, second-stage retrieval.

tacl_a_00530.pdf	Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Ques- tion Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evalu- ate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowl- edge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxil- iary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant in- formation from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the re- triever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve sig- nificant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work’s credibility and technical consistency.

2308.11761v1.pdf	Large language models (LLMs) have demon- strated impressive impact in the field of nat- ural language processing, but they still strug- gle with several issues regarding, such as com- pleteness, timeliness, faithfulness and adapt- ability. While recent efforts have focuses on connecting LLMs with external knowledge sources, the integration of knowledge bases (KBs) remains understudied and faces several challenges. In this paper, we introduce Knowl- edGPT, a comprehensive framework to bridge LLMs with various knowledge bases, facili- tating both the retrieval and storage of knowl- edge. The retrieval process employs the pro- gram of thought prompting, which generates search language for KBs in code format with pre-defined functions for KB operations. Be- sides retrieval, KnowledGPT offers the capa- bility to store knowledge in a personalized KB, catering to individual user demands. With ex- tensive experiments, we show that by integrat- ing LLMs with KBs, KnowledGPT properly answers a broader range of questions requiring world knowledge compared with vanilla LLMs, utilizing both knowledge existing in widely- known KBs and extracted into personalized KBs.

2310.05149v1.pdf	Large language models augmented with task-relevant docu- ments have demonstrated impressive performance on knowl- edgeintensive tasks. However, regarding how to obtain effec- tive documents, the existing methods are mainly divided into two categories. One is to retrieve from an external knowledge base, and the other is to utilize large language models to gen- erate documents. We propose an iterative retrieval-generation collaborative framework. It is not only able to leverage both parametric and non-parametric knowledge, but also helps to find the correct reasoning path through retrieval-generation interactions, which is very important for tasks that require multi-step reasoning. We conduct experiments on four ques- tion answering datasets, including single-hop QA and multi- hop QA tasks. Empirical results show that our method signif- icantly improves the reasoning ability of large language mod- els and outperforms previous baselines. Index Terms— large language models, retrieval aug- mented, question answering

2404.14851v3.pdf	—Information Retrieval (IR) systems are crucial tools for users to access information, widely applied in scenarios like search engines, question answering, and recommendation systems. Traditional IR methods, based on similarity matching to return ranked lists of documents, have been reliable means of information acquisition, dominating the IR field for years. With the advancement of pre-trained language models, generative information retrieval (GenIR) has emerged as a novel paradigm, gaining increasing attention in recent years. Currently, research in GenIR can be categorized into two aspects: generative document retrieval (GR) and reliable response generation. GR leverages the generative model’s parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing. Reliable response generation, on the other hand, employs language models to directly generate the information users seek, breaking the limitations of traditional IR in terms of document granularity and relevance matching, offering more flexibility, efficiency, and creativity, thus better meeting practical needs. This paper aims to systematically review the latest research progress in GenIR. We will summarize the advancements in GR regarding model training, document identifier, incremental learning, downstream tasks adaptation, multi-modal GR and generative recommendation, as well as progress in reliable response generation in aspects of internal knowledge memorization, external knowledge augmentation, generating response with citations and personal information assistant. We also review the evaluation, challenges and future prospects in GenIR systems. This review aims to offer a comprehensive reference for researchers in the GenIR field, encouraging further development in this area. Index Terms—Generative Information Retrieval; Generative Document Retrieval; Reliable Response Generation ✦

information-14-00356-v2.pdf	The metaverse represents an immersive digital environment that has garnered signiﬁcant attention as a result of its potential to revolutionize various industry sectors and its profound societal impact. While academic interest in the metaverse has surged, a dearth of comprehensive review articles employing bibliometric techniques remains. This study seeks to address this gap by analyzing

2306.16092v2.pdf	AI legal assistants based on Large Language Models (LLMs) can provide accessible legal consulting services, but the hallucination problem poses potential legal risks. This paper presents Chatlaw, an innovative legal assistant utilizing a Mixture-of-Experts (MoE) model and a multi-agent system to enhance the reliability and accuracy of AI-driven legal services. By integrating knowledge graphs with artificial screening, we construct a high-quality legal dataset to train the MoE model. This model utilizes different experts to address various legal issues, optimizing the accuracy of legal responses. Additionally, Standardized Operating Procedures (SOP), modeled after real law firm workflows, significantly reduce errors and hallucinations in legal services. Our MoE model outperforms GPT-4 in the Lawbench and Unified Qualification Exam for Legal Professionals by 7.73% in accuracy and 11 points, respectively, and also surpasses other models in multiple dimensions during real-case consultations, demonstrating our robust capability for legal consultation. Keywords Artificial Intelligence in Law, Mixture of Experts, Large Language Model, Knowledge Graph, Legal Technology Legal services play a crucial role in protecting individual rights and maintaining social fairness1–3. However, the limited availability of legal professionals and the high cost of their services often restrict access to these services, particularly in China, with its vast population and extensive social interactions. Statistics show that Chinese legal aid centers have accepted 800,000 cases over the past seven years, aiding over 6 million people, yet annually, only one-quarter of the 700,000 cases received can be processed4. This gap in legal service provision deeply impacts justice and equity, especially for those lacking the resources to effectively navigate the legal system. This raises a crucial question: can we establish an automated legal assistant to address these challenges? In recent years, the efficacy of LLMs has been validated across multiple scientific fields, encompassing natural language processing5,6, biochemistry7–14, and the medical field15–22. LLMs also offer potential solutions to the challenges in legal services. Popular models like ChatGPT23 and the LLaMA24 series, along with other general-purpose25–30 or law-specific models,31,32 can respond to user inputs based on their internal legal knowledge repositories and provide advisory recommendations. However, the inherent hallucination issues in LLMs pose potential risks in their application to legal domains33 since they operate at the level of word distributions rather than validated facts34. The knowledge generated by these models is often incomplete or outdated, leading them to produce illusions that, although seemingly relevant, may be misleading or incorrect33,35. To address the mentioned issues, this paper designs Chatlaw, a multi-agent virtual legal assistant based on a multi-expert large language model. Chatlaw effectively mitigates hallucination issues through key aspects of data quality, model optimization, and consulting processes. Initially, we create a high-quality legal dataset through multiple screenings and integrate similar advisory knowledge into a knowledge graph to ensure data accuracy. Next, we expand from a single-expert model to an MoE model, increasing the parameter space to allow different experts to handle specific advisory tasks, thus enhancing the accuracy of the legal consult. Building on this, we emulate the service workflows of real law firms and develop a set of SOP for multi-agent collaboration. This SOP includes four independent intelligent agent roles responsible for initial information gathering, in-depth material research, legal advice, and final consultation report writing. This procedural operation ensures each step of information processing is efficient and accurate, significantly enhancing the quality of legal services and client

3701228.pdf	ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Copyright held by the owner/author(s). ACM 1558-2868/2024/10-ART ACM Trans. Inf. Syst.

