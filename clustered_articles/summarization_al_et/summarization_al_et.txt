Filename	Abstract
s10506-023-09345-y.pdf	The appropriate understanding and fast processing of lengthy legal documents are computationally challenging problems. Designing efficient automatic summariza- tion techniques can potentially be the key to deal with such issues. Extractive sum- marization is one of the most popular approaches for forming summaries out of such lengthy documents, via the process of summary-relevant sentence selection. An effi- cient application of this approach involves appropriate scoring of sentences, which helps in the identification of more informative and essential sentences from the document. In this work, a novel sentence scoring approach DCESumm is proposed which consists of supervised sentence-level summary relevance prediction, as well as unsupervised clustering-based document-level score enhancement. Experimental results on two legal document summarization datasets, BillSum and Forum of Infor- mation Retrieval Evaluation (FIRE), reveal that the proposed approach can achieve significant improvements over the current state-of-the-art approaches. More specifi- cally it achieves ROUGE metric F1-score improvements of (1−6)% and (6−12)% for the BillSum and FIRE test sets respectively. Such impressive summarization results suggest the usefulness of the proposed approach in finding the gist of a lengthy legal document, thereby providing crucial assistance to legal practitioners. Keywords Legal document summarization · Extractive summarization · Legal BERT · Deep clustering * Deepali Jain deepali_rs@cse.nits.ac.in Malaya Dutta Borah malayaduttaborah@cse.nits.ac.in Anupam Biswas anupam@cse.nits.ac.in

3543507.3583197.pdf	Extractive summarization helps provide a short description or a di­ gest of news or other web texts. It enhances the reading experience of users, especially when they are reading on small displays (e.g., mobile phones). Matching-based methods are recently proposed for the extractive summarization task, which extracts a summary from a global view via a document-summary matching framework. However, these methods only calculate similarities between candi­ date summaries and the entire document embeddings, insufciently capturing interactions between diferent contextual information in the document to accurately estimate the importance of candidates. In this paper, we propose a new hyperbolic interaction model for extractive multi-document summarization (HISum). Specifcally, HISum frst learns document and candidate summary representa­ tions in the same hyperbolic space to capture latent hierarchical structures and then estimates the importance scores of candidates by jointly modeling interactions between each candidate and the document from global and local views. Finally, the importance scores are used to rank and extract the best candidate as the ex­ tracted summary. Experimental results on several benchmarks show that HISum outperforms the state-of-the-art extractive baselines1. CCS CONCEPTS • Computing methodologies → Information extraction. KEYWORDS Extractive Summarization, Multi-document Summarization, Repre­ sentation Learning, Hyperbolic Deep Learning ACM Reference Format: Mingyang Song, Yi Feng, and Liping Jing. 2023. HISum: Hyperbolic Interac­ tion Model for Extractive Multi-Document Summarization. In Proceedings of the ACM Web Conference 2023 (WWW ’23), April 30–May 04, 2023, Austin, TX, USA. ACM, New York, NY, USA, 10 pages.

pone.0304057.pdf	Automatic Text Summarization (ATS) is gaining popularity as there is a growing demand for a system capable of processing extensive textual content and delivering a concise, yet meaningful, relevant, and useful summary. Manual summarization is both expensive and time-consuming, making it impractical for humans to handle vast amounts of data. Conse- quently, the need for ATS systems has become evident. These systems encounter chal- lenges such as ensuring comprehensive content coverage, determining the appropriate length of the summary, addressing redundancy, and maintaining coherence in the gener- ated summary. Researchers are actively addressing these challenges by employing Natural Language Processing (NLP) techniques. While traditional methods exist for generating summaries, they often fall short of addressing multiple aspects simultaneously. To over- come this limitation, recent advancements have introduced multi-objective evolutionary algorithms for ATS. This study proposes an enhancement to the performance of ATS through the utilization of an improved version of the Binary Multi-Objective Grey Wolf Opti- mizer (BMOGWO), incorporating mutation. The performance of this enhanced algorithm is assessed by comparing it with state-of-the-art algorithms using the DUC2002 dataset. Experimental results demonstrate that the proposed algorithm significantly outperforms the compared approaches.

2301.12652v4.pdf	We introduce REPLUG, a retrieval-augmented lan- guage modeling framework that treats the lan- guage model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language mod- els with special cross attention mechanisms to en- code the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be eas- ily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the perfor- mance of Codex on five-shot MMLU by 5.1%.

2310.04408v1.pdf	Retrieving documents and prepending them in-context at inference time improves performance of language model (LMs) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual sum- maries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of LMs to identify relevant information in long re- trieved documents. We present two compressors – an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs’ performance on end tasks when the generated summaries are prepended to the LMs’ input, while keeping the summary concise. If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, implementing selective augmentation. We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as

s10506-024-09411-z.pdf	Automatic summarization of legal case judgements, which are known to be long and complex, has traditionally been tried via extractive summarization models. In recent years, generative models including abstractive summarization models and Large lan- guage models (LLMs) have gained huge popularity. In this paper, we explore the applicability of such models for legal case judgement summarization. We applied various domain-specific abstractive summarization models and general-domain LLMs as well as extractive summarization models over two sets of legal case judge- ments – from the United Kingdom (UK) Supreme Court and the Indian Supreme Court – and evaluated the quality of the generated summaries. We also perform experiments on a third dataset of legal documents of a different type – Government reports from the United States. Results show that abstractive summarization models and LLMs generally perform better than the extractive methods as per traditional metrics for evaluating summary quality. However, detailed investigation shows the presence of inconsistencies and hallucinations in the outputs of the generative mod- els, and we explore ways to reduce the hallucinations and inconsistencies in the summaries. Overall, the investigation suggests that further improvements are needed to enhance the reliability of abstractive models and LLMs for legal case judgement summarization. At present, a human-in-the-loop technique is more suitable for per- forming manual checks to identify inconsistencies in the generated summaries. Keywords Legal judgement summarization · Abstractive summarization · Large language models · Prompting · Hallucinations * Aniket Deroy roydanik18@kgpian.iitkgp.ac.in Kripabandhu Ghosh kripa.ghosh@gmail.com Saptarshi Ghosh saptarshi.ghosh@gmail.com

2401.18059v1.pdf	Retrieval-augmented language models can better adapt to changes in world state and incorporate long-tail knowledge. However, most existing methods retrieve only short contiguous chunks from a retrieval corpus, limiting holistic under- standing of the overall document context. We introduce the novel approach of recursively embedding, clustering, and summarizing chunks of text, constructing a tree with differing levels of summarization from the bottom up. At inference time, our RAPTOR model retrieves from this tree, integrating information across lengthy documents at different levels of abstraction. Controlled experiments show that retrieval with recursive summaries offers significant improvements over tra- ditional retrieval-augmented LMs on several tasks. On question-answering tasks that involve complex, multi-step reasoning, we show state-of-the-art results; for example, by coupling RAPTOR retrieval with the use of GPT-4, we can improve the best performance on the QuALITY benchmark by 20% in absolute accuracy.

2308.08973v2.pdf	Multi-hop question answering (QA) involves finding multiple relevant passages and step-by- step reasoning to answer complex questions, indicating a retrieve-and-read paradigm. How- ever, previous retrievers were customized for two-hop questions, and most of them were trained separately across different hops, re- sulting in a lack of supervision over the en- tire multi-hop retrieval process and leading to poor performance in complicated scenarios be- yond two hops. In this work, we introduce Beam Retrieval, an end-to-end beam retrieval framework for multi-hop QA. This approach models the multi-hop retrieval process in an end-to-end manner by jointly optimizing an encoder and two classification heads across all hops. Moreover, Beam Retrieval main- tains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. To establish a complete QA system, we incorporate a supervised reader or a large language model (LLM). Experimental results demonstrate that Beam Retrieval achieves a nearly 50% improvement compared with base- lines on challenging MuSiQue-Ans, and it also surpasses all previous retrievers on HotpotQA and achieves 99.9% precision on 2WikiMulti- HopQA. Providing high-quality context, Beam Retrieval helps our supervised reader achieve new state-of-the-art performance and substan- tially improves the few-shot QA performance of LLMs1.

s10115-024-02195-3.pdf	Summarizing extensive documents involves selecting sentences, with the organizational structure of document sections playing a pivotal role. However, effectively utilizing dis- course information for summary generation poses a signiﬁcant challenge, especially given the inconsistency between training and evaluation in extractive summarization. In this paper, we introduce GoSum, a novel extractive summarizer that integrates a graph-based model with reinforcement learning techniques to summarize long documents. Speciﬁcally, GoSum utilizes a graph neural network to encode sentence states, constructing a heterogeneous graph that represents each document at various discourse levels. The edges of this graph capture hierarchical relationships between different document sections. Furthermore, GoSum incor- porates ofﬂine reinforcement learning, enabling the model to receive ROUGE score feedback on diverse training samples, thereby enhancing the quality of summary generation. On the two scientiﬁc article datasets PubMed and arXiv, GoSum achieved the highest performance among extractive models. Particularly on the PubMed dataset, GoSum outperformed other models with ROUGE-1 and ROUGE-L scores surpassing by 0.45 and 0.26, respectively. Keywords Extractive summarization · Graph neural network · Reinforcement learning · Long document summarization

2306.05317v1.pdf	In this paper, we consider the challenge of sum- marizing patients’ medical progress notes in a limited data setting. For the Problem List Summarization (shared task 1A) at the BioNLP Workshop 2023, we demonstrate that Clinical- T5 fine-tuned to 765 medical clinic notes out- performs other extractive, abstractive and zero- shot baselines, yielding reasonable baseline sys- tems for medical note summarization. Further, we introduce Hierarchical Ensemble of Summa- rization Models (HESM), consisting of token- level ensembles of diverse fine-tuned Clinical- T5 models, followed by Minimum Bayes Risk (MBR) decoding. Our HESM approach lead to a considerable summarization performance boost, and when evaluated on held-out chal- lenge data achieved a ROUGE-L of 32.77, which was the best-performing system at the top of the shared task leaderboard.1

2023.findings-acl.400.pdf	Pre-trained abstractive summarization models can generate fluent summaries and achieve high ROUGE scores. Previous research has found that these models often generate summaries that are inconsistent with their context docu- ment and contain nonfactual information. To evaluate factuality in document summariza- tion, a document-level Natural Language In- ference (NLI) classifier can be used. How- ever, training such a classifier requires large- scale high-quality factual and nonfactual sam- ples. To that end, we introduce NonFactS, a data generation model to synthesize nonfac- tual summaries given a context document and a human-annotated (reference) factual summary. Compared to previous methods, our nonfactual samples are more abstractive and more simi- lar to their corresponding factual samples, re- sulting in state-of-the-art performance on two factuality evaluation benchmarks, FALSESUM and SUMMAC. Our experiments demonstrate that even without human-annotated summaries, NonFactS can use random sentences to gen- erate nonfactual summaries and a classifier trained on these samples generalizes to out-of- domain documents.1

2407.01370v1.pdf	LLMs and RAG systems are now capable of handling millions of input tokens or more. However, evaluating the output quality of such systems on long-context tasks remains chal- lenging, as tasks like Needle-in-a-Haystack lack complexity. In this work, we argue that summarization can play a central role in such evaluation. We design a procedure to synthe- size Haystacks of documents, ensuring that spe- cific insights repeat across documents. The “Summary of a Haystack” (SummHay) task then requires a system to process the Haystack and generate, given a query, a summary that identifies the relevant insights and precisely cites the source documents. Since we have pre- cise knowledge of what insights should appear in a haystack summary and what documents should be cited, we implement a highly re- producible automatic evaluation that can score summaries on two aspects – Coverage and Citation. We generate Haystacks in two do- mains (conversation, news), and perform a large-scale evaluation of 10 LLMs and corre- sponding 50 RAG systems. Our findings indi- cate that SummHay is an open challenge for current systems, as even systems provided with an Oracle signal of document relevance lag our estimate of human performance (56%) by 10+ points on a Joint Score. Without a retriever, long-context LLMs like GPT-4o and Claude 3 Opus score below 20% on SummHay. We show SummHay can also be used to study enterprise RAG systems and position bias in long-context models. We hope future systems can equal and surpass human performance on SummHay.

2021.emnlp-main.333.pdf	Most of existing extractive multi-document summarization (MDS) methods score each sentence individually and extract salient sen- tences one by one to compose a summary, which have two main drawbacks: neglect- ing both the intra and cross-document relations between sentences; neglecting the coher- ence and conciseness of the whole summary. In this paper, we propose a novel MDS frame- work (SgSum) to formulate the MDS task as a sub-graph selection problem, in which source documents are regarded as a relation graph of sentences (e.g., similarity graph or discourse graph) and the candidate summaries are its sub- graphs. Instead of selecting salient sentences, SgSum selects a salient sub-graph from the relation graph as the summary. Comparing with traditional methods, our method has two main advantages: the relations between sentences are captured by modeling both the graph structure of the whole document set and the candidate sub-graphs; directly out- puts an integrate summary in the form of sub- graph which is more informative and coher- ent. Extensive experiments on MultiNews and DUC datasets show that our proposed method brings substantial improvements over several strong baselines. Human evaluation results also demonstrate that our model can produce signiﬁcantly more coherent and informative summaries compared with traditional MDS

2310.00785v4.pdf	Summarizing book-length documents (>100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: ex- isting book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture er- rors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: hierarchically merging chunk-level summaries, and incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because hu- man evaluation is expensive and time-consuming, we develop an automatic met- ric, BOOOOKSCORE, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BOOOOKSCORE has high agree- ment with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving $15K USD and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BOOOOKSCORE than those generated by open-source models. While LLaMA 2 falls behind other mod- els, Mixtral achieves performance on par with GPT-3.5-Turbo. Incremental up- dating yields lower BOOOOKSCORE but higher level of detail than hierarchical merging, a trade-off sometimes preferred by annotators. We release code and annotations to spur more principled research on book-length summarization. github.com/lilakk/BooookScore

2305.11859v2.pdf	Retrieving evidence to support or refute claims is a core part of automatic fact-checking. Prior work makes simplifying assumptions in re- trieval that depart from real-world use cases: either no access to evidence, access to evidence curated by a human fact-checker, or access to evidence published after a claim was made. In this work, we present the first realistic pipeline to check real-world claims by retrieving raw evidence from the web. We restrict our re- triever to only search documents available prior to the claim’s making, modeling the realistic scenario of emerging claims. Our pipeline in- cludes five components: claim decomposition, raw document retrieval, fine-grained evidence retrieval, claim-focused summarization, and veracity judgment. We conduct experiments on complex political claims in the CLAIMDE- COMP dataset and show that the aggregated evidence produced by our pipeline improves veracity judgments. Human evaluation finds the evidence summary produced by our system is reliable (it does not hallucinate information) and relevant to answering key questions about a claim, suggesting that it can assist fact-checkers even when it does not reflect a complete evi- dence set.1

2305.11841v1.pdf	Popularized by the Differentiable Search In- dex, the emerging paradigm of generative re- trieval re-frames the classic information re- trieval problem into a sequence-to-sequence modeling task, forgoing external indices and encoding an entire document corpus within a single Transformer. Although many differ- ent approaches have been proposed to improve the effectiveness of generative retrieval, they have only been evaluated on document cor- pora on the order of 100k in size. We conduct the ﬁrst empirical study of generative retrieval techniques across various corpus scales, ulti- mately scaling up to the entire MS MARCO passage ranking task with a corpus of 8.8M passages and evaluating model sizes up to 11B parameters. We uncover several ﬁndings about scaling generative retrieval to millions of pas- sages; notably, the central importance of using synthetic queries as document representations during indexing, the ineffectiveness of existing proposed architecture modiﬁcations when ac- counting for compute cost, and the limits of naively scaling model parameters with respect to retrieval performance. While we ﬁnd that generative retrieval is competitive with state- of-the-art dual encoders on small corpora, scal- ing to millions of passages remains an impor- tant and unsolved challenge. We believe these ﬁndings will be valuable for the community to clarify the current state of generative retrieval, highlight the unique challenges, and inspire new research directions.

2309.04087v1.pdf	Multi-document summarization aims to obtain core information from a collection of docu- ments written on the same topic. This pa- per proposes a new holistic framework for un- supervised multi-document extractive summa- rization. Our method incorporates the holistic beam search inference method associated with the holistic measurements, named Subset Rep- resentative Index (SRI). SRI balances the im- portance and diversity of a subset of sentences from the source documents and can be calcu- lated in unsupervised and adaptive manners. To demonstrate the effectiveness of our method, we conduct extensive experiments on both small and large-scale multi-document summa- rization datasets under both unsupervised and adaptive settings. The proposed method outper- forms strong baselines by a significant margin, as indicated by the resulting ROUGE scores and diversity measures. Our findings also sug- gest that diversity is essential for improving multi-document summary performance.

2307.05696v1.pdf	that captures the most salient aspects. However, research on this topic often neglects the usefulness of the approach for users, focusing primarily on the accuracy of the generated summaries. As a result, these approaches produce short (3- arXiv:2307.05696v1 [cs.IR] 9 Jul 2023

2404.19543v1.pdf	Large Language Models (LLMs) have cat- alyzed significant advancements in Natural Language Processing (NLP), yet they en- counter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have in- tegrated information retrieved from external resources with LLMs, substantially enhanc- ing their performance across NLP tasks. This survey paper addresses the absence of a com- prehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval- Augmented Generation (RAG) and Retrieval- Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evo- lution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, ac- curacy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computa- tional efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their po- tential, and the avenues for their future develop- ment in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https: //github.com/2471023025/RALM_Survey.

2309.09369v2.pdf	Previous research in multi-document news sum- marization has typically concentrated on col- lating information that all sources agree upon. However, the summarization of diverse infor- mation dispersed across multiple articles about an event remains underexplored. In this pa- per, we propose a new task of summarizing diverse information encountered in multiple news articles encompassing the same event. To facilitate this task, we present a data collection schema for identifying diverse information and curated a dataset named DIVERSESUMM. The dataset includes 245 news stories, with each story comprising 10 news articles and paired with a human-validated reference. Next, to enable consistent automatic evaluation, we con- duct a comprehensive analysis to pinpoint the position and verbosity biases when utilizing Large Language Model (LLM)-based metrics for evaluating the coverage and faithfulness of summaries. Through correlation analyses, we outline the best practices for effectively using automatic LLM-based metrics on the DIVERS- ESUMM dataset. Finally, we study how LLMs summarize multiple news articles by analyz- ing which type of diverse information LLMs are capable of identifying. Our analyses sug- gest that despite the extraordinary capabilities of LLMs in single-document summarization, the proposed task remains a complex challenge for them mainly due to their limited coverage, with GPT-4 only able to cover under 40% of the diverse information on average.1

electronics-12-01895.pdf	generation [2,5–7]. Extractive MDS aims to mark important sentences in the document cluster and create the summary in such a manner that repetition of the different documents does not result in the duplication of information in the summary. The summary should also not miss important facts found in the multiple documents, while handling the issue of redundancy . There are several challenges that extractive MDS suffers from. One such challenge is the poor grammaticality of the resultant summary . A lot of research is currently focusing on improving the sentence quality of the summary. Since input documents may contain similar content in different words and formats, it is important to process words and phrases that have a similar meaning in a similar way. This can be achieved using Electronics 2023, 12, 1895

2310.03025v2.pdf	Extending the context window of large language models (LLMs) is getting popular recently, while the solution of augmenting LLMs with retrieval has existed for years. The natural questions are: i) Retrieval-augmentation versus long context window, which one is better for downstream tasks? ii) Can both methods be combined to get the best of both worlds? In this work, we answer these questions by studying both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context window using simple retrieval-augmentation at generation can achieve compa- rable performance to finetuned LLM with 16K context window via positional interpolation on long context tasks, while taking much less computation. More importantly, we demonstrate that retrieval can significantly improve the perfor- mance of LLMs regardless of their extended context window sizes. Our best model, retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5- turbo-16k and Davinci003 in terms of average score on nine long context tasks including question answering, query-based summarization, and in-context few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a margin, while being much faster at generation. Our study provides general insights on the choice of retrieval-augmentation versus long context extension of LLM for practitioners.

s11042-023-14613-9.pdf	With the rapid growth of social media platforms, digitization of official records, and digital publication of articles, books, magazines, and newspapers, lots of data are generated every day. This data is a foundation of information and contains a vast amount of text that may be complex, ambiguous, redundant, irrelevant, and unstructured. There- fore, we require tools and methods that can help us understand and automatically summarize the vast amount of generated text. There are mainly two types of approaches to perform text summarization: abstractive and extractive. In Abstractive Text Summa- rization, a concise summary is generated by including the salient features of the input documents and paraphrasing documents using new sentences and phrases. While in Extractive Text Summarization, a summary is produced by selecting and combining the most significant sentences and phrases from the source documents. The researchers have given numerous techniques for both kinds of text summarization. In this work, we classify Extractive Text Summarization approaches and review them based on their characteristics, techniques, and performance. We have discussed the existing Extractive Text Summarization approaches along with their limitations. We also classify and discuss evaluation measures and provide the research challenges faced in Extractive Text Summarization. * Avaneesh Kumar Yadav avaneesh17@mnnit.ac.in Ranvijay ranvijay@mnnit.ac.in Rama Shankar Yadav rsy@mnnit.ac.in Ashish Kumar Maurya ashishmaurya@mnnit.ac.in

2310.03414v1.pdf	Multi-document summarization is a challenging task due to its inherent subjective bias, highlighted by the low inter- annotator ROUGE-1 score of 0.4 among DUC-2004 refer- ence summaries. In this work, we aim to enhance the objec- tivity of news summarization by focusing on the main event of a group of related news documents and presenting it co- herently with sufficient context. Our primary objective is to succinctly report the main event, ensuring that the summary remains objective and informative. To achieve this, we em- ploy an extract-rewrite approach that incorporates a main- event biased monotone-submodular function for content se- lection. This enables us to extract the most crucial informa- tion related to the main event from the document cluster. To ensure coherence, we utilize a fine-tuned Language Model (LLM) for rewriting the extracted content into a coherent text. The evaluation using objective metrics and human evaluators confirms the effectiveness of our approach, as it surpasses potential baselines, demonstrating excellence in both content coverage, coherence, and informativeness.

2305.13693v1.pdf	Evaluating multi-document summarization (MDS) quality is difficult. This is especially true in the case of MDS for biomedical liter- ature reviews, where models must synthesize contradicting evidence reported across different documents. Prior work has shown that rather than performing the task, models may exploit shortcuts that are difficult to detect using stan- dard n-gram similarity metrics such as ROUGE. Better automated evaluation metrics are needed, but few resources exist to assess metrics when they are proposed. Therefore, we introduce a dataset of human-assessed summary quality facets and pairwise preferences to encourage and support the development of better auto- mated evaluation methods for literature review MDS. We take advantage of community sub- missions to the Multi-document Summariza- tion for Literature Review (MSLR) shared task to compile a diverse and representative sam- ple of generated summaries. We analyze how automated summarization evaluation metrics correlate with lexical features of generated sum- maries, to other automated metrics including several we propose in this work, and to aspects of human-assessed summary quality. We find that not only do automated metrics fail to cap- ture aspects of quality as assessed by humans, in many cases the system rankings produced by these metrics are anti-correlated with rankings according to human annotators.1

tacl_a_00605.pdf	Retrieval-Augmented Language Modeling (RALM) methods, which condition a language model (LM) on relevant documents from a grounding corpus during generation, were shown to significantly improve language modeling performance. In addition, they can mitigate the problem of factually inac- curate text generation and provide natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper considers a simple alternative, which we dub In-Context RALM: leaving the LM architecture unchanged and prepending grounding documents to the input, without any further training of the LM. We show that In-Context RALM that builds on off-the-shelf general purpose retrievers provides surpris- ingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that In-Context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access.1

1211305.pdf	The exponential growth of textual data across various domains necessitates the development of eﬃcient and accurate summa- rization techniques to facilitate quick comprehension and information retrieval. The novel automated system for summarizing multiple document abstracts and titles using advanced neural architectures addresses this need by leveraging large language models to generate concise and coherent summaries. The methodology involved comprehensive data collection, preprocessing, model selection, and summarization processes, evaluated through a combination of quantitative and qualitative metrics. Re- sults demonstrated high eﬃcacy in handling shorter documents and strong performance in technical domains such as healthcare and science, although challenges in coherence and readability were noted. Domain-speciﬁc performance highlighted the ne- cessity for tailored adaptations, and the study contributed valuable insights into hybrid summarization techniques combining extractive and abstractive methods. Future research directions include the development of advanced attention mechanisms, domain-speciﬁc ﬁne-tuning, and reinforcement learning techniques to optimize summarization quality, alongside addressing ethical considerations to ensure responsible deployment.

2024.naacl-long.96.pdf	Multi-hop question answering (QA) involves finding multiple relevant passages and step-by- step reasoning to answer complex questions, indicating a retrieve-and-read paradigm. How- ever, previous retrievers were customized for two-hop questions, and most of them were trained separately across different hops, re- sulting in a lack of supervision over the en- tire multi-hop retrieval process and leading to poor performance in complicated scenarios be- yond two hops. In this work, we introduce Beam Retrieval, an end-to-end beam retrieval framework for multi-hop QA. This approach models the multi-hop retrieval process in an end-to-end manner by jointly optimizing an encoder and two classification heads across all hops. Moreover, Beam Retrieval main- tains multiple partial hypotheses of relevant passages at each step, expanding the search space and reducing the risk of missing relevant passages. To establish a complete QA system, we incorporate a supervised reader or a large language model (LLM). Experimental results demonstrate that Beam Retrieval achieves a nearly 50% improvement compared with base- lines on challenging MuSiQue-Ans, and it also surpasses all previous retrievers on HotpotQA and achieves 99.9% precision on 2WikiMulti- HopQA. Providing high-quality context, Beam Retrieval helps our supervised reader achieve new state-of-the-art performance and substan- tially improves the few-shot QA performance of LLMs1.

24649_Knowledge_guided_Aspect_.pdf	from a source document with only the contents that are relevant to a speciﬁc aspect. It ﬁrst emerged in the con- sumer feedback domain, where the system extracts information regarding product properties and feedback sentiment from customers . Recently, investigated several methods to induce latent topical information into sequence-to-sequence abstractive summarization frameworks. They have also created a dataset for aspect-based summarization. Our work is closely related to theirs, using the same dataset as theirs. However, different from their work, we attempt to inject external knowl- edge to guide the model to generate more topical-related words when generating summaries instead of using only the document information. D. Knowledge Graph A Knowledge Graph (KG) is a multi-relational graph com- posed of entities (nodes) and relations (different types of edges). Each edge is represented as a triple of the form (head entity, relation, tail entity), also called a fact, indicating that two entities are connected by a speciﬁc relation, e.g., (Al- fredHitchcock, DirectorOf, Psycho). It has been widely used for representing factual knowledge in downstream applications such as word sense disambiguation , question answering , and information extraction . In our experiments, we choose ConceptNet as our knowledge graph. It is a multilingual knowledge base representing words and phrases that people use and the common-sense relationships between them. The knowledge in ConceptNet is collected from a variety of resources, including crowdsourced resources (such as Wiktionary and Open Mind Common Sense), games with a purpose (such as Verbosity and nadya.jp), and expert-created resources (such as WordNet and JMDict). III. PRELIMINARIES A. Problem Deﬁnition We formalize the problem of aspect-based summarization as below. Given a document X which contains n tokens x1, x2, ..., xn and an aspect a, our system produces a summary Y which contains m tokens y1, y2, ..., ym that satisfy the following requirements: m < n, Y only contains contents that are relevant to a. B. Base Model We chose BART as our baseline model, aiming to improve its performance by adding different knowledge. BART is a sequence-to-sequence model pre-trained using denoising objectives and has shown superior performance in text-to-text problems such as text summarization and machine translation. IV. PROPOSED METHOD The overall architecture of our model is shown in Fig. 1. It follows the architecture of BART, and we investigate both knowledge graph and sequence level score as knowledge sources to train the model. BART Encoder BART Decoder The United States … force. … … surprise attack on … … … Document Summary Knowledge Graph War Attack Battle …… …… Aspect: War attack Additional Reward BART BART Encoder BART Decoder BART The United States … force. … Document The US entered … … In 1941, the US … … Sampling Beam Search Sequence-level Supervision Knowledge Source 1 Knowledge Source 2 Fig. 1. Proposed methods. We introduce two knowledge sources into BART: one is through a knowledge graph, and the other is through human-deﬁned sequence-level scores.

paper_85.pdf	Fine-tuning a pretrained model for downstream tasks is a widely-adopted technique, which is known for its adaptability and reliability across various domains. Despite its conceptual simplicity, fine-tuning entails several engineering choices such as the selection of hyperparameters and the determination of checkpoints from an optimization trajectory. To tackle the difficulty of choosing the best model among multiple ones obtained from those choices, one of the effective solutions is model fusion, which combines multiple models on a parameter space. On the other hand, we observe a large discrepancy between loss and actual metric values where a loss is often used to pick out models to fuse. While the loss is generally differentiable and thus easier to optimize, the consideration of metrics is often a preferable goal to improve model performance. In response, we present a novel model fusion technique, optimizing a desired metric as well as a loss using Bayesian Optimization (BO). Moreover, combining the multi-objective BO into model fusion, we devise a bilevel framework, composed of BO models for hyperparameter optimization and model fusion. Experiments across various downstream tasks validate decent performance improvements achieved using our BO-based model fusion method.

26537-Article Text-30600-1-2-20230626.pdf	Multi-document summarization (MDS) aims to generate a summary for a number of related documents. We propose HGSUM — an MDS model that extends an encoder-decoder architecture to incorporate a heterogeneous graph to repre- sent different semantic units (e.g., words and sentences) of the documents. This contrasts with existing MDS models which do not consider different edge types of graphs and as such do not capture the diversity of relationships in the documents. To preserve only key information and relationships of the documents in the heterogeneous graph, HGSUM uses graph pooling to compress the input graph. And to guide HGSUM to learn the compression, we introduce an additional objec- tive that maximizes the similarity between the compressed graph and the graph constructed from the ground-truth sum- mary during training. HGSUM is trained end-to-end with the graph similarity and standard cross-entropy objectives. Experimental results over MULTI-NEWS, WCEP-100, and ARXIV show that HGSUM outperforms state-of-the-art MDS models. The code for our model and experiments is available at:

2310.08319v1.pdf	The effectiveness of multi-stage text retrieval has been solidly demonstrated since before the era of pre-trained language models. However, most existing studies utilize models that pre- date recent advances in large language models (LLMs). This study seeks to explore poten- tial improvements that state-of-the-art LLMs can bring. We conduct a comprehensive study, fine-tuning the latest LLaMA model both as a dense retriever (RepLLaMA) and as a point- wise reranker (RankLLaMA) for both passage retrieval and document retrieval using the MS MARCO datasets. Our findings demonstrate that the effectiveness of large language mod- els indeed surpasses that of smaller models. Additionally, since LLMs can inherently han- dle longer contexts, they can represent entire documents holistically, obviating the need for traditional segmenting and pooling strategies. Furthermore, evaluations on BEIR demonstrate that our RepLLaMA–RankLLaMA pipeline ex- hibits strong zero-shot effectiveness. Model checkpoints from this study are available on HuggingFace.1

