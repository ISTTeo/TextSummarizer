Filename	Abstract
2311.15800v1.pdf	In order to uncover users' attitudes towards ChatGPT in mental health, this study examines public opinions about ChatGPT in mental health discussions on Reddit. Researchers used the bert-base-multilingual-uncased-sentiment techniques for sentiment analysis and the BERTopic model for topic modeling. It was found that overall, negative sentiments prevail, followed by positive ones, with neutral sentiments being the least common. The prevalence of negative emotions has increased over time. Negative emotions encompass discussions on ChatGPT providing bad mental health advice, debates on machine vs. human value, the fear of AI, and concerns about Universal Basic Income (UBI). In contrast, positive emotions highlight ChatGPT's effectiveness in counseling, with mentions of keywords like "time" and "wallet." Neutral discussions center around private data concerns. These findings shed light on public attitudes toward ChatGPT in mental health, potentially contributing to the development of trustworthy AI in mental health from the public perspective.

2212.08632v2.pdf	Multi-modal multi-hop question answering involves answering a question by reasoning over multiple input sources from different modalities. Existing methods often retrieve evidences separately and then use a language model to generate an answer based on the retrieved evidences, and thus do not adequately connect candi- dates and are unable to model the interdependent relations during retrieval. Moreover, the pipelined approaches of retrieval and gen- eration might result in poor generation performance when retrieval performance is low. To address these issues, we propose a Structured Knowledge and Unified Retrieval-Generation (SKURG) approach. SKURG employs an Entity-centered Fusion Encoder to align sources from different modalities using shared entities. It then uses a unified Retrieval-Generation Decoder to integrate intermediate retrieval results for answer generation and also adaptively determine the number of retrieval steps. Extensive experiments on two representa- tive multi-modal multi-hop QA datasets MultimodalQA and WebQA demonstrate that SKURG outperforms the state-of-the-art models in both source retrieval and answer generation performance with fewer parameters1. CCS CONCEPTS â€¢ Computing methodologies â†’Natural language processing; Computer vision; Knowledge representation and reasoning. KEYWORDS Question Answering, Cross-modal Reasoning, Multi-modal Re- trieval âˆ—Corresponding author.

2022.constraint-1.8.pdf	During the COVID-19 pandemic, the spread of misinformation on online social media has grown exponentially. Unverified bogus claims on these platforms regularly mislead people, leading them to believe in half-baked truths. The current vogue is to employ man- ual fact-checkers to verify claims to combat this avalanche of misinformation. However, establishing such claimsâ€™ veracity is becom- ing increasingly challenging, partly due to the plethora of information available, which is dif- ficult to process manually. Thus, it becomes imperative to verify claims automatically with- out human interventions. To cope up with this issue, we propose an automated claim verification solution encompassing two steps â€“ document retrieval and veracity prediction. For the retrieval module, we employ a hybrid search-based system with BM25 as a base re- triever and experiment with recent state-of-the- art transformer-based models for re-ranking. Furthermore, we use a BART-based textual entailment architecture to authenticate the re- trieved documents in the later step. We report experimental findings, demonstrating that our retrieval module outperforms the best baseline system by 10.32 NDCG@100 points. We es- cort a demonstration to assess the efficacy and impact of our suggested solution. As a byprod- uct of this study, we present an open-source, easily deployable, and user-friendly Python API that the community can adopt.

2305.11719v2.pdf	Existing research on multimodal relation ex- traction (MRE) faces two co-existing chal- lenges, internal-information over-utilization and external-information under-exploitation. To combat that, we propose a novel frame- work that simultaneously implements the idea of internal-information screening and external- information exploiting. First, we represent the fine-grained semantic structures of the input im- age and text with the visual and textual scene graphs, which are further fused into a unified cross-modal graph (CMG). Based on CMG, we perform structure refinement with the guid- ance of the graph information bottleneck prin- ciple, actively denoising the less-informative features. Next, we perform topic modeling over the input image and text, incorporating latent multimodal topic features to enrich the contexts. On the benchmark MRE dataset, our system outperforms the current best model significantly. With further in-depth analyses, we reveal the great potential of our method for the MRE task. Our codes are open at

2311.01449v2.pdf	Topic modeling is a well-established technique for exploring text corpora. Conventional topic models (e.g., LDA) represent topics as bags of words that often require â€œreading the tea leavesâ€ to interpret; additionally, they offer users mini- mal control over the formatting and specificity of resulting topics. To tackle these issues, we in- troduce TopicGPT, a prompt-based framework that uses large language models (LLMs) to un- cover latent topics in a text collection. Top- icGPT produces topics that align better with human categorizations compared to competing

2310.08975v3.pdf	Knowledge Base Question Answering (KBQA) aims to answer natural language questions over large-scale knowledge bases (KBs), which can be summarized into two crucial steps: knowledge retrieval and semantic parsing. However, three core challenges remain: in- efficient knowledge retrieval, mistakes of re- trieval adversely impacting semantic parsing, and the complexity of previous KBQA meth- ods. To tackle these challenges, we introduce ChatKBQA, a novel and simple generate-then- retrieve KBQA framework, which proposes first generating the logical form with fine- tuned LLMs, then retrieving and replacing en- tities and relations with an unsupervised re- trieval method, to improve both generation and retrieval more directly. Experimental re- sults show that ChatKBQA achieves new state- of-the-art performance on standard KBQA datasets, WebQSP, and CWQ. This work can also be regarded as a new paradigm for combin- ing LLMs with knowledge graphs (KGs) for in- terpretable and knowledge-required question answering. Our code is publicly available1.

2305.01750v2.pdf	Question answering over knowledge bases is considered a difï¬cult problem due to the chal- lenge of generalizing to a wide variety of pos- sible natural language questions. Additionally, the heterogeneity of knowledge base schema items between different knowledge bases often necessitates specialized training for different knowledge base question-answering (KBQA) datasets. To handle questions over diverse KBQA datasets with a uniï¬ed training-free framework, we propose KB-BINDER, which for the ï¬rst time enables few-shot in-context learning over KBQA tasks. Firstly, KB- BINDER leverages large language models like Codex to generate logical forms as the draft for a speciï¬c question by imitating a few demon- strations. Secondly, KB-BINDER grounds on the knowledge base to bind the generated draft to an executable one with BM25 score match- ing. The experimental results on four public heterogeneous KBQA datasets show that KB- BINDER can achieve a strong performance with only a few in-context demonstrations. Es- pecially on GraphQA and 3-hop MetaQA, KB- BINDER can even outperform the state-of-the- art trained models. On GrailQA and WebQSP, our model is also on par with other fully- trained models. We believe KB-BINDER can serve as an important baseline for future re- search. Our code is available at https:// github.com/ltl3A87/KB-BINDER

s10462-022-10254-w.pdf	Social media platforms such as (Twitter, Facebook, and Weibo) are being increasingly embraced by individuals, groups, and organizations as a valuable source of information. This social media generated information comes in the form of tweets or posts, and nor- mally characterized as short text, huge, sparse, and low density. Since many real-world applications need semantic interpretation of such short texts, research in Short Text Topic Modeling (STTM) has recently gained a lot of interest to reveal unique and cohesive latent topics. This article examines the current state of the art in STTM algorithms. It presents a comprehensive survey and taxonomy of STTM algorithms for short text topic modelling. The article also includes a qualitative and quantitative study of the STTM algorithms, as well as analyses of the various strengths and drawbacks of STTM techniques. Moreover, a comparative analysis of the topic quality and performance of representative STTM models is presented. The performance evaluation is conducted on two real-world Twitter datasets: the Real-World Pandemic Twitter (RW-Pand-Twitter) dataset and Real-world Cyberbully- ing Twitter (RW-CB-Twitter) dataset in terms of several metrics such as topic coherence, purity, NMI, and accuracy. Finally, the open challenges and future research directions in this promising field are discussed to highlight the trends of research in STTM. The work presented in this paper is useful for researchers interested in learning state-of-the-art short text topic modelling and researchers focusing on developing new algorithms for short text topic modelling. Keywords Big data Â· Social media Â· Short text topic modeling Â· Data streaming Â· Coherence Â· Sparseness Â· Deep learning topic modeling * Belal Abdullah Hezam Murshed belal.a.hezam@gmail.com Extended author information available on the last page of the article

173.pdf	With the improvement of hardware computing power and the development of deep learning algorithms, a revolution of â€œartificial intelligence (AI) + medical imageâ€ is taking place. Benefiting from diversified modern medical measurement equipment, a large number of medical images will be pro- duced in the clinical process. These images improve the diagnostic accuracy of doctors, but also increase the labor burden of doctors. Deep learning technology is expected to realize an auxiliary diagnosis and improve diagnostic efficiency. At present, the method of deep learning technology combined with atten- tion mechanism is a research hotspot and has achieved state-of-the-art results in many medical image tasks. This paper reviews the deep learning attention methods in medical image analysis. A comprehen- sive literature survey is first conducted to analyze the keywords and literature. Then, we introduce the development and technical characteristics of the attention mechanism. For its application in medical image analysis, we summarize the related methods in medical image classification, segmentation, detec- tion, and enhancement. The remaining challenges, potential solutions, and future research directions are also discussed. Keywords: medical image; attention mechanism; deep learning

applsci-13-00797.pdf	Topic modelling is a prominent task for automatic topic extraction in many applications such as sentiment analysis and recommendation systems. The approach is vital for service industries to monitor their customer discussions. The use of traditional approaches such as Latent Dirichlet Allocation (LDA) for topic discovery has shown great performances, however, they are not consistent in their results as these approaches suffer from data sparseness and inability to model the word order in a document. Thus, this study presents the use of Kernel Principal Component Analysis (KernelPCA) and K-means Clustering in the BERTopic architecture. We have prepared a new dataset using tweets from customers of Nigerian banks and we use this to compare the topic modelling approaches. Our ï¬ndings showed KernelPCA and K-means in the BERTopic architecture-produced coherent topics with a coherence score of 0.8463. language processing; banking industry; Nigeria Pidgin English

SURVEY-1.pdf	- Survey of probabilistc topic models is presented with emphasis on fundamentally different approaches used in modeling. Introduced classification differs from earlier efforts, providing a complementary view of the field. Purpose of this survey is to provide a brief overview of the current probailistic topic models as well as an inspiration for future research.

2408.04216v1.pdf	â€”This paper advances a novel architectural schema anchored upon the Transformer paradigm and innovatively amalgamates the K-means categorization algorithm to augment the contextual apprehension capabilities of the schema. The transformer model performs well in machine translation tasks due to its parallel computing power and multi-head attention mechanism. However, it may encounter contextual ambiguity or ignore local features when dealing with highly complex language structures. To circumvent this constraint, this exposition incorporates the K-Means algorithm, which is used to stratify the lexis and idioms of the input textual matter, thereby facilitating superior identification and preservation of the local structure and contextual intelligence of the language. The advantage of this combination is that K-Means can automatically discover the topic or concept regions in the text, which may be directly related to translation quality. Consequently, the schema contrived herein enlists K-Means as a preparatory phase antecedent to the Transformer and recalibrates the multi-head attention weights to assist in the discrimination of lexis and idioms bearing analogous semantics or functionalities. This ensures the schema accords heightened regard to the contextual intelligence embodied by these clusters during the training phase, rather than merely focusing on locational intelligence. Keywordsâ€”Transformer; Text mining; Machine translation; K-Means I. INTRODUCTION This paper aims to explore an innovative approach to improve context modeling and understanding in the text mining and machine translation domains by combining the Transformer architecture with the K-Means clustering algorithm. The Transformer paradigm has risen to dominance in the realm of machine translation since its inception, primarily due to its exceptional parallel processing capabilities and multi- head attention mechanism. This architecture enables the model to process the entire input sequence simultaneously, significantly enhancing processing speed and improving translation accuracy. However, despite its proficiency in managing long-range dependencies, the Transformer model may sometimes overlook local features, resulting in contextual ambiguity or inaccurate translations, especially when dealing with technical terminology or culturally specific expressions. To tackle the aforementioned challenges, this discourse introduces an innovative architectural construct denominated as the k-Transformer, which amalgamates the K-Means cluster analysis algorithm into the Transformer framework to augment the context-sensitive acumen of the model. The k- Transformer operates by leveraging the K-Means algorithm to aggregate the input textual data, pinpointing and safeguarding the local configuration of lexemes and idioms bearing analogous semantics or functionalities before model training. This preparatory phase empowers the model to discern thematic or conceptual domains within the textual content more effectively, thereby focusing increased attention on the contextual nuances of these domains during the translation operation. The K-Means algorithm is an unsupervised learning technique, proficient in autonomously unveiling the inherent structure embedded within the dataset. Within the k- Transformer construct, K-Means is harnessed to categorize the input textual matter at the lexical and idiomatic levels, amalgamating similar entities together. Consequently, the model apprehends essential conceptual territories even within extended textual sequences, avoiding sole reliance on positional indices for attention distribution. This refined attention mechanism enables the model to translate textual matter encompassing intricate contexts and specialized nomenclature with greater precision, thereby elevating the overall quality of the translation. The conventional evaluative metric, the BLEU score, is employed to quantify the translation fidelity and preservation of contextual integrity in the experimental setting. The K-Transformer framework proposed in this paper brings new possibilities to the field of machine translation and text mining by fusing the Transformer architecture and the K- Means clustering algorithm. This method not only improves the model's ability to understand complex contexts, but also enhances the accuracy and fluency of translation. The K- Transformer framework is highly beneficial across several fields, particularly in enhancing natural language processing applications. It can significantly improve machine translation, information retrieval, and sentiment analysis, facilitating deeper understanding and generation of text. Furthermore, it's useful in developing financial risk management and multi-scale image recognition, and for document summarization tasks, aiding in efficient information extraction. Additionally, in healthcare, K- Transformers can assist in interpreting medical texts, supporting diagnostic and treatment processes, and enriching medical research literature analysis. This versatility makes K- Transformers valuable in any sector where advanced language comprehension is required. II. RELATED WORK The field of machine translation and text mining has seen significant advancements with the integration of deep learning models, particularly with the Transformer architecture. The Transformer model, renowned for its parallel processing capabilities and multi-head attention mechanism, has revolutionized natural language processing tasks, including machine translation. Despite its effectiveness, the model can sometimes miss local features, leading to contextual ambiguities. This gap is addressed by Liu et al., who propose a convolutional neural network-based feature extraction model to enhance local feature detection in complex language structures . Similarly, Yang et al. investigate the use of deep learning for diagnostic applications, highlighting the model's proficiency in recognizing intricate patterns within textual data. This methodology's relevance extends to machine translation, where understanding nuanced contextual elements is crucial . Efficiency optimization in large-scale language models, as explored by Mei et al., is directly relevant to improving the performance of the k-Transformer framework. Their research emphasizes the need for balancing model complexity and computational efficiency . Similarly, Li et al. discuss various approaches to visual question answering, providing insights into handling diverse query types and enhancing model robustness . These studies collectively inform the development of hybrid models like the k-Transformer by highlighting the importance of computational efficiency and robust model architecture. Further emphasizing the importance of integrating multiple data sources to improve contextual understanding, Liu et al. discuss the application of multimodal fusion deep learning models in disease recognition. This approach aligns with the k-Transformer's strategy of combining clustering algorithms with deep learning to enhance contextual understanding . Xu et al. advance the predictive capabilities of models in financial contexts, offering insights that could inform the development of hybrid models like the k-Transformer . Additionally, Gao et al. present an enhanced network architecture for reducing information loss, relevant for preserving context in text mining and translation tasks . Finally, advanced prompt engineering techniques are demonstrated by Ding et al., who enhance model outputs in complex tasks, showing the potential of these techniques in improving translation accuracy and fluency . Zhan et al. introduce innovations in temporal expression recognition using LSTM networks, providing a framework for understanding temporal context, which is crucial for accurate translation . Yang et al. extend the concept to emotional analysis, highlighting the importance of nuanced context comprehension in achieving accurate results . III. TRANSFORMER MODEL WITH SELF-ATTENTION MECHANISM The core component of the Transformer is its encoder module, which is composed of a series of stacked encoder layers with a consistent architecture. Embedded in each layer are two key components: Multi-head Attention and a fully connected feedforward neural network. The model architecture is shown in Figure 1. Figure 1 Transformer Model Architecture In order to ensure that the model can still learn effectively when the depth is increased, the designer cleverly adds a residual connection strategy between the sub-layers, which not only helps to alleviate the problem of gradient disappearance, but also further optimizes the gradient propagation path through the subsequent Layer Normalization operation, which significantly accelerates the training convergence of the model. As shown in Equation 1. layernorm(â„+ sublayer(â„)) Where layernorm(Â·) signifies the output of the layer regularization function, and sublayer(Â·) represents the emanation of the sublayer; h is the symbolic representation of the covert status of the sublayer input. The decoder is fabricated from a tier of n congruent decoder strata. Contrasted against the encoder, the decoder integrates an encoder-decoder cross-attention subsidiary stratum to amalgamate the data from both the encoder and the decoder. Once more, a residual linkage is employed betwixt each dyad of subsidiary strata, succeeded by a stratum of normalization operation. The foundational component of the multi-headed scrutiny within the encoder and decoder subsidiary strata of the Transformer is the Scaled Dot-product scrutiny, which ingests the query matrix Q alongside the key- value duet matrices K and V as ingress. The conditioning procedure of the dot product scrutiny mechanism unfolds as follows. Post the embedding of lexical units and positional indicators, the originating sentence acquires the ingress sequence ğ‘‹= (ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘›) âˆˆâ„ğ‘‘ğ‘¥. We delineate trio matrices: ğ‘Šğ‘œ,ğ‘Šğ¾,ğ‘Šğ‘‰ utilizing these matrices for respective linear transformations of the ingress sequence, and thenceforth engendering trio neoteric vectors: ğ‘ğ‘¡,ğ‘˜ğ‘¡,ğ‘£ğ‘¡. Concatenate all ğ‘ğ‘¡ vectors into a voluminous matrix, denominated as the query matrix Q; concatenate all ğ‘˜ğ‘¡ vectors into a voluminous matrix, denominated as the key matrix K; and concatenate all ğ‘£ğ‘¡ vectors into a voluminous matrix, denominated as the value matrix V. The scrutiny weight of the inaugural lexical unit is procured by multiplying the query vector ğ‘1 of the inaugural lexical unit with the key matrix K. Subsequently, we necessitate the application of a softmax function to the values to ensure their summation equals unity. The output of the inaugural lexical unit is attained by multiplying each weight by the weighted aggregation of the value vector ğ‘£ğ‘¡ of the corresponding lexical unit. Continuation of the aforementioned operations upon the sequential ingress vectors yields all outputs subsequent to traversing the dot product scrutiny mechanism, as delineated in Equation 2. Attention(ğ‘„, ğ¾, ğ‘‰) = softmax (ğ‘„ğ¾ğ‘‡ âˆšğ‘‘ğ‘˜ ) ğ‘‰ Ultimately, the polycephalous attention mechanism synthesizes the outcomes of numerous dot product scrutiny operations to attain the terminal output: â„ğ‘’ğ‘ğ‘‘ğ‘–= Attention(ğ‘„ğ‘Šğ‘„, ğ¾ğ‘Šğ¾, ğ‘‰ğ‘Šğ‘‰) MultiHead(ğ‘„, ğ¾, ğ‘‰) = Concat(â„ğ‘’ğ‘ğ‘‘ğ‘–)ğ‘Šğ‘œ Where: ğ‘Šğ‘„ ,ğ‘Šğ¾, ğ‘Šğ‘‰, ğ‘Šğ‘œ are parameter matrices. â„ğ‘’ğ‘ğ‘‘ğ‘– is the output vector of the ith attention head. The feedforward neural network is a fully connected network layer between two layers, and RELU is used as the activation function, as shown in Equation 5. FFN(ğ‘¥) = ğ‘šğ‘ğ‘¥(0, ğ‘¥ğ‘¾1 + ğ‘1)ğ‘¾2 + ğ‘2 Here, ğ‘¾1, ğ‘¾2, ğ‘1, ğ‘2 are the model parameters. Owing to the fact that the Transformer lacks recurrent and convolutional neural architectures, it operates upon each term within the sentence concurrently and is devoid of the capacity to discern the sequential arrangement of individual terms. Incorporating positional encoding serves to aid the transference paradigm in recognizing the locational metadata associated with terms within the sentence. The magnitude of the positional encoding aligns identically with the dimensionality of the term vector, and the amalgamation of these can subsequently serve as the ingress sequence. The Transformer employs trigonometric sine and cosine functions, characterized by varying periodicity, to calculate the positional encoding, as delineated in Equations 6 and 7. PE(ğ‘ƒğ‘‚ğ‘†,2ğ‘–) = sin ( ğ‘ğ‘œğ‘ 

