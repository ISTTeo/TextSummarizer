[
    {
        "file_path": "/cfs/home/u024236/Documents/RandomPDFsEpub/Guide for Paper Writing.pdf",
        "metadata": {
            "/Author": "Felso Ganhane",
            "/CreationDate": "D:20240428203406+01'00'",
            "/ModDate": "D:20240428203406+01'00'",
            "/Producer": "Microsoft: Print To PDF",
            "/Title": "Excerpts from Appendix from Stephen Van...l Science (Ithaca, NY_ Cornell Univers"
        },
        "text": "",
        "summary": ""
    },
    {
        "file_path": "/cfs/home/u024236/Documents/RandomPDFsEpub/Graph_Neural_Network_GNN_in_Image_and_Video_Understanding_Using_Deep_Learning_for_Computer_Vision_Applications.pdf",
        "metadata": {
            "/Meeting Starting Date": "4 Aug. 2021",
            "/ModDate": "D:20210914162742-04'00'",
            "/IEEE Article ID": "9532631",
            "/CreationDate": "D:20210811173740Z",
            "/IEEE Issue ID": "9532596",
            "/Producer": "PDF Editor 2.2 - Foxit Corporation; modified using iText\u00ae 7.1.1 \u00a92000-2018 iText Group NV (AGPL-version)",
            "/Subject": "2021 Second International Conference on Electronics and Sustainable Communication Systems (ICESC);2021; ; ;10.1109/ICESC51422.2021.9532631",
            "/IEEE Publication ID": "9532475",
            "/Title": "Graph Neural Network (GNN) in Image and Video Understanding Using Deep Learning for Computer Vision Applications",
            "/Meeting Ending Date": "6 Aug. 2021"
        },
        "text": "Graph Neural Network (GNN) in Image and Video \nUnderstanding Using Deep Learning for Computer \nVision Applications  \n \nPradhyumna P, Shreya G P, Mohana  \nElectronics and Telecommunication Engineering,  RV College of Engineering\u00ae  \nBengaluru , Karnataka, India. \n \nAbstract - Graph neural networks (GNNs) is an information - \nprocessing system that uses message passing among graph \nnodes. In recent years, GNN variants including graph attention \nnetwork (GAT), graph convolutional network (GCN), and  \ngraph recurrent network (GRN) have shown revolutionary \nperformance in computer vision applications using deep  \nlearning and artificial intelligence. These neural network model \nextensions, collect information in the form of graphs. GNN may \nbe divided into three groups based on the challenges it solves: \nlink prediction, node classification, graph classification. \nMachines can differentiate and recognise objects in image and  \nvideo using standard CNNs. Extensive amount of research work \nneeds to be done before robots can have same visual intuition as \nhumans. GNN architectures, on the other hand, may be used to \nsolve various image categorization and video challenges. The \nnumber of GNN applications in computer vision not limited, \ncontinues to expand. Human-object interaction, actin \nunderstanding, image categorization from a few shots and many \nmore. In this paper use of GNN in image and video \nunderstanding, design aspects, architecture, applications and  \nimplementation challenges towards computer vision is \ndescribed. GNN is a strong tool for analysing graph data and is \nstill a relatively active ar ea that needs further research es \nattention to solve many computer vision applications. \nKeywords \u2014 Graph Neural Networks (GNNs), Convolutional \nNeural Network (CNN), Gated Adversarial Transformer (GAT), \nAtomic Visual Action(AVA), Human-Object Interactions (HOI), \nGraph Parsing Neural Network (GPNN). \nI. INTRODUCTION  \nGraphs are one of the most useful data structures for various \napplications areas such as learning cell fingerprints, exploring \ntraffic networks, prescribing companions to interpersonal \ninteraction and modeling body systems. Non-Euclidean \ngraphs need to be addressed by these activities which contain \ndetails of interpersonal relationships which can be mishandled \nby using deep traditional learning models. Nodes in graphs \nregularly contain valuable data that is ignored in profoundly \nmoderated unregulated learning techniques. GNNs are \nproposed to consolidate the graph structure and feature details \nto pursue better presentations on graphs with integration and \nfeature distribution. Because of its persuading execution and \nhigh interpretation, GNN has as of late become broadly \nutilized. GNNs can model the relationship of nodes of the \ngraph and generate a numerical representation of it. GNNs are \nparticularly essential since there is so much factual \ninformation that can be expressed also as graphs. Social networks, chemical compounds, maps, and \ntransportation systems are just a few examples. Due to \nadvancements in DL, in variety of image comprehension \ntasks, including image categorization, object recognition, as \nwell as semantic segmentation. Deep learning algorithms' \ncapacity to learn at many levels of abstraction from data \naccounts for this resounding success. Different levels of \nabstraction are required for different activities. This \ngeneralized image classification challenge helps determine \nwhich object classes are included in a given image (usually \nfrom a collection of predefined classifications). Image \nclassification methodology based on GNN models is \nevolving, since GNN, which derive their motivation from \nCNN, are used in this domain. If given a large training dataset \nof labelled class, many of these models, including GNN, \nprovide interesting results. Recent video understanding \ndatasets such as AVA and Charades, on the other hand, have \nlagged behind in comparison. Understanding the interactions \nbetween actors, objects, and other context in a scene is one of \nthe many reasons why video comprehension is very \nimportant. Furthermore, because these interactions aren't \nalways visible in a single frame, reasoning over large time \nintervals is required. Because video has an additional \ntemporal axis, it has a much higher dimensional signal than \nsingle images, and we believe that learning these unlabeled \ninteractions directly from current datasets with huge \nconvolutional networks is not possible. Effective video \ncomprehension necessitates long-term reasoning about the \nlinks between objects, actors and their surroundings. Because \ngraph-structured data is ubiquitous, it can be employed in a \nwide range of scenarios. \nII. LITERATURE SURVEY   \nAnurag Arnab [1] describes how to make a proposal when \nsupervision is not available, they suggest a message-passing \nGNN that can use explicit representations of objects and \nclearly represents these spatio-temporal interconnections. \nThis strategy is demonstrated on two different challenges in \nvideo spatio-temporal action identification that require \nrelational thinking. It also demonstrates how this method may \nmore successfully model relationships between s ignificant \nthings in images, numerically  and qualitatively. Natu ral video \nevents are usually outcome of spatio-temporal engagements \namong actors and objects, and they frequently involve a large \nnumber of object types. Yubo Zhang [2] proposes a study that Proceedings of the Second International Conference on Electronics and Sustainable Communication Systems (ICESC-2021)\nIEEE Xplore Part Number: CFP21V66-ART; ISBN: 978-1-6654-2867-5\n978-1-6654-2867-5/21/$31.00 \u00a92021 IEEE 11832021 Second International Conference on Electronics and Sustainable Communication Systems (ICESC) | 978-1-6654-2867-5/21/$31.00 \u00a92021 IEEE | DOI: 10.1109/ICESC51422.2021.9532631\nAuthorized licensed use limited to: b-on: Universidade de Lisboa Reitoria. Downloaded on May 06,2024 at 13:26:30 UTC from IEEE Xplore.  Restrictions apply. \nargues that action detection is a difficult problem to solve \nsince the models which must be trained are massive, and \nobtaining tagged data is costly. To overcome this constraint, \nthey recommend incorporating domain knowledge into the \nmodel's structure to make optimization easier. Santiago \nCastro [3] presents the majority of research work on \nlanguage-assisted video understanding has centered on two \ntasks: firstly, usage of multiple choice questions for video \nquestion answering, here models perform well because \ncandidate solutions are easily available. Second, to capture a \nvideo which uses an open-ended assessment framework. They \nsuggest fill in the blanks as just a video comprehension \nassessment framework that corrects previous assessment \nproblems but more closely matches real-life circumstances in \nwhich many possibilities are unavailable. Using this \nassociated text and video, the model must predict a concealed \nnoun phrase inside this video description, which assesses the \nsystem's knowledge of the film. The dataset is built from the \nVATEX dataset, with blurred captions generated by stamping \nnoun phrases in the English captioning in VATEX. To \nconstruct an instance, we select the first English caption that \nusually contains one noun phrase as recognized by spaCy1, \nthen blank these kinds of nouns at random. As a result, we \nstart only with the VATEX v1.1 training set, a randomized \nsubset of size 1000 from the test dataset, respectively, to \nconstruct our training, validating, and testing data. To acquire \nadditional right answers for each space in the verification and \ntest sets, we used a crowd annotating technique. The key aim \nfor gathering such additional annotations, as previously said, \nis to account for the diversity of words and to have several \nalternatives for each space.  K. Sasabuchi [4] presents a \nlearning-from-observation framework for extracting precise \naction sequences from a video of a human demonstration split \nand understood with vocal instructions. Splitting is based on \nminimum local points in hand velocity, which link human \ndaily movements with object-centered facial contact \ntransitions required for robot motion generation. They first \nestablished that hand velocity motion splitting is a reliable \nsignal for partitioning daily tasks. Secondly, they generated a \nnew motion description dataset with the goal of better \nunderstanding everyday human actions. Also provides the \ninformation of attention-based models, researchers developed \nGated Adversarial Transformer (GAT). To increase the \nmodel's performance even further, they applied adversarial \ntraining methodologies. A regularization term was added to \nthe loss function, giving the model adversarial robustness to \nboth attention mappings and the final output space.  Matthew \nHutchinson [5] describes deep learning research in computer \nvision is a natural extension of video understanding. The \napplication of artificial neural network (ANN) machine \nlearning (ML) approaches has tremendously benefitted the \nfield of image interpretation. They grouped deep learning \nmodel building blocks and state- of-the-art model families, \nand specified standard metrics for assessing models. They \nalso listed datasets suitable as benchmarks and pre-training \nsources, discussed data preparation stages and techniques, and \norganized deep learning model building blocks and state- of-\nthe-art model families. Ishan Dave [6] proposes a temporal \ncontrastive learning framework outperforms state of the art \noutcomes in a variety of downstream video comprehension \ntasks, including action recognition, limited-label action \nclassification, and action classification and nearest-neighbor video retrieval on several videos and datasets. On three \ndifferent datasets, this paper gives comprehensive \nexperimental evidence and achieves best- in-class results in a \nrange of downstream video interpretation tasks. Beyond \ninstance discrimination, the success of our methodology \ndemonstrates the advantages of contrastive learning.  \nA. Gupta [7] proposes a simple idea which offers world \nfeatures, a basic concept in which each feature at each layer \nhas its own spatial transformation, and the feature map is only \naltered when needed. Results show that a network created \nwith these World Features may be utilised to mimic eye \nmovements including saccades, fixation, and smooth pursuit \non pre-recorded video in a batch environment. It finds that \nnumerous eye movements are achievable, allowing for a wide \nrange of augmentations without sacrificing relative feature \nposition. They utilised these concepts to supply the model \nwith a transformation for each video in the experiments, but \nlearnt transformations might also be employed. Yubo Zhang \n[8] proposes a notion that action detection is a difficult \nproblem to solve based on the fact that the models are large to \nbe trained and obtaining labelled data is costly. To overcome \nthis issue, they suggest incorporating domain expertise into \nthe model's structure to make optimization easier. This model \nsurpasses existing best practices, proving the method's utility \nin modelling spatial correlation and reasoning about linkages. \nMost significantly, the performance of this model highlights \nthe need of incorporating relational and temporal knowledge \ninto design methods for detection of action. H. Huang [9 ] \ndescribes a new dynamic hidden graph module in videos for \nmodelling complicated object-object interactions, with two \ninstantiations: a visual graph for capturing appearance/motion \nchanges among objects and a location graph to capture \nrelative spatiotemporal position changes among objects. The \nsuggested graph module may explicitly capture interactions \namong objects in streaming video contexts by evaluating \nobject relations at the same time both in time domain and \nfrequency domain, which distinguishes our work from prior \ntechniques. \nY. Chen [ 10] presents a novel way of explanation that \naggregates a collection of features globally across the \ncoordinate space before moving to an interacting area \nwhereby relationship thinking may be calculated successfully. \nJ. Zhou [11] describes many learning activities including \nworking with graph data, which offers a wealth of relational \ninformation between parts. The usage of a model that can \nlearn from graph inputs is required for simulating the physical \nsystems, establishing fingerprints, predicting proteins \ninterface, and diagnosing illnesses. They did a thorough \nexamination of graph neural networks. They divide GNN \nmodels into variants based on compute modules, graph kinds, \nand training kinds. They also describe a number of general \nframeworks and present a number of theoretical studies. F. \nScarselli [12] proposed GNN model, for processing data it \nextends existing neural network approaches shown in this \ngraph domain. The mentioned GNN model can handle most \ncommon graph topologies directly, including acyclic, cyclical, \nguided, and unguided graphs. The paradigm revolves upon \ninformation diffusion and relaxation processes. The generic \nframework, prior generative method for organized data \nprocessing, and approaches based on vague walk mod are all \nincorporated into the approach. K. Simonyan [13] proposes \nthe 2 stream ConvNet design that includes both spatial and Proceedings of the Second International Conference on Electronics and Sustainable Communication Systems (ICESC-2021)\nIEEE Xplore Part Number: CFP21V66-ART; ISBN: 978-1-6654-2867-5\n978-1-6654-2867-5/21/$31.00 \u00a92021 IEEE 1184\nAuthorized licensed use limited to: b-on: Universidade de Lisboa Reitoria. Downloaded on May 06,2024 at 13:26:30 UTC from IEEE Xplore.  Restrictions apply. \ntemporal networks. First they show that despite minimal \ntraining samples, the results of a ConvNet taught on multi \nframe intensive optical flow might be great. Finally, they \ndemonstrate how multitask learning may be utilised to \nimprove the quantity of data collected from two separate \naction classification datasets. \nR. Girdhar [14] introduces a method for recognizing and \nlocalizing living beings in video footage, use the Actions \nConverter model. They employ a transformer architecture for \ncollecting characteristics from the spatiotemporal \nenvironment around the individual whose behaviors are being \nclassified. They demonstrated that even the Actions Converter \nnetwork can acquire spatiotemporal information from several \nother human Behaviour and items inside a film clip and use it \nto recognize and localize human activities. Siyuan Qi [15] \ndescribes the challenge of identifying and distinguishing \nhuman-object interactions (HOI) in photos and videos in this \nwork. A Graph Parsing Neural Network (GPNN) is \nintroduced, an end- to-end differentiable architecture that \nincorporates structural information. They test this model on \nvarious data sets such as V-COCO, HICO-DET and CAD-\n120, which on photos and videos, are all HOI recognition \nstandards. This technique outperforms current methods, \nindicating that GPNN was adaptable to large datasets. And \ncan be used in both spatial and temporal scenarios. Boncelet \n[16] describes testing of the proposed method's performance, \nwhere two picture understanding tasks were chosen: Emotion \nrecognition at the group level and incident identification both \ntask is extremely meaningful, and synthesizing multiple cues \nnecessitates the interplay of numerous deep models. \nUnderstanding an image includes not just recognising the \nitems in it, but also grasping their fundamental relationships \nand interconnections. GNNs may take advantage of such \nlinkages during the feature learning and forecasting phases by \nspreading nodal messages through the network and \naggregating the outputs. Danfei Xu [17] suggests the use of \nscene graphs, a graphical framework for a picture that is \nvisually anchored, to formally model the objects and their \ninteractions. They also propose a revolutionary end- to-end \nparadigm for creating structured scene representations  from \nan input image. They developed a novel end to end model that \nsolves the challenge of automatically constructing a visibly \nanchored virtual environment from an image by continuous \npassing of messages between the primal and dual sub-graphs \nalong the topological structure of a scene graph. Yubo Zhang \n[18] describes Action detection as an example of a difficult \nproblem: the models that must be trained are enormous, yet \nlabelled data is difficult to get by. To overcome this \nconstraint, they recommend incorporating domain knowledge \ninto the model's structure to make optimization easier. The \nsuggested methodology outperforms the by 5.5 percent mAP \nin the I3D base and 4.8 percent mAP on AVA dataset. \nIII. GRAPH NEURAL NETWORK MODEL AND \nARCHITECTURE  FOR  IMAGE AND VIDEO \nUNDRSTANDING  \nImage categorization, a classic computer vision problem, \nwhere convolutional neural networks (CNN) being the most \nprominent one. GNNs, which get their motivation from CNN, \nhave been used in this arena as well. Main goal is to improve \nzero-shot and few-shot learning task models performance. \nZero shot learning (ZSL) is the process of training a model to \nrecognize classes it has never seen before. ZSL image categorization is to control structural information. As a result, \nGNN appears to be highly tempting in this regard. The \ninformation needed to lead the ZSL work may be found in \nknowledge graphs. The type of information that each \ntechnique represents in the graph varies by knowledge. \nGraphs of such kind may well be built on commonalities \nbetween both the photos themselves or those of the objects \nrecovered using object recognition in the photos. Semantic \ninformation from embeddings of the image class labels may \nalso be included in the graphs. GNNs may then be used to \nenhance the ZSL picture classification-recognition process by \napplying them to this structured data. The process of creating \na label for a video based on its frames is video classification, \nstrong video level classification not only delivers correct \nframe labels, and also best represents the entire movie based \non the characteristics and annotation of the individual frames. \nThe process of creating a label for a video based on its frames \nis video classification. A strong video level classification not \nonly delivers correct frame labels, and also best represents the \nentire movie based on the characteristics and annotation of the \nindividual frames. Paper [1], propose a message passing \ngraph GNN to spatio-temporal interactions and for object \nrepresentations it uses explicit object if monitoring is \navailable else implicit object shall be used. Their approach \nbroadens earlier structured models for video comprehension, \nallowing us to investigate how varied graph  representation \nand structure choices impact the model's performance. This \nshows how to apply a strategy to two separate tasks in videos \nthat require related reasoning \u2013 on AVA and UCF101-24 it \nuses an action detection model of spatio temporal relation, on \nthe recently released Action Genome dataset it uses video \nscene graph categorization on dataset. It also demonstrates \nthat this strategy may more successfully model relationships \nbetween significant things in the picture, both numerically \nand qualitatively. \n \nFig.1. Graph-structured data representation [ 19]  \nFigure 1 shows the graph-structured data to understand the \nactivity of video. The visual ST graph with unique edge types, \nactor- to-actor temporal, object- to-actor spatial etc., and \nunique node types is a heterogeneous graph with varied \nsemantics and dimensions. First, it describes objects and \nactors visual spatio-temporal interactions. Second, co-\noccurrences, for example, are a common connection between \nlabels. These signals can be represented visually and \nProceedings of the Second International Conference on Electronics and Sustainable Communication Systems (ICESC-2021)\nIEEE Xplore Part Number: CFP21V66-ART; ISBN: 978-1-6654-2867-5\n978-1-6654-2867-5/21/$31.00 \u00a92021 IEEE 1185\nAuthorized licensed use limited to: b-on: Universidade de Lisboa Reitoria. Downloaded on May 06,2024 at 13:26:30 UTC from IEEE Xplore.  Restrictions apply. \nsymbolically in a mixed spatial temporal and symbolic \nattributed graphs. This hybrid graph to conduct supervised \nlearning on recognized semantic elements, such as objects and \nactors, to create perspective models that may be used to tackle \nsubsequent video processing tasks [ 19].  \n \nFig.2. Action detection in video sequence [2] \nFigure 2 shows the action detection in video sequence (AVA \ndataset).Person rising from their seat and collecting a letter \nfrom some other person seated beside a table. Information is \ngenuinely necessary for recognising and localising this \nactivity out of the 2359296 pixels inside the 36 frames of this \nsnap, the actor's motion, location and interactions with other \nactors and the text are all important indications. The rest of \nthe video's data, such as the wall color or the light on the \ntable, is extraneous and should be ignored. Action region \ndetection on such intuitive insights, it\u2019s vital to collect both \ndeep temporal features and spatial interactions across actors \nand objects when detecting actions [2] .  \n \nFig.3. Action understanding [5] \nAction issues, video action data, data processing approaches, \ndeep learning models, and assessment measures all fall under \nthe umbrella of action understanding as shown in figure 3 [5] . \nThe ideas of computing performance, data variety, \ntraceability, model resilience, and readability underpin these \nprocesses in computer vision and deep learning. Summary of \naction phases (dataset selection, problem formation, model \nconstruction, dataset preparation, and metrics basis \nevaluation) as well as core assumptions (data diversity, \ncomputational performance, robustness, transferability and \nunderstandability).  \nFig.4. Object \u2013 to - object interaction [9] \nFigure 4 shows object to object interaction, mainly two \nrelations should be considered for recognising such \ninteractions: first, interactions between various images into a \nsingle frame. Second, transitions of such interactions between \ndifferent items and then the same item across successive \nframes [9]. The former is referred to as a spatial relationship, \nwhereas the latter is referred to as a temporal relationship. \nBoth are necessary for recognising multi-object operations. \nAn efficient strategic recognition model will accurately and \nconcurrently capture both relationships. \n \nFig.5. Action detection and interaction framework [2]. \nFigure 5 shows the action detection and interaction \nframework, it receives a video frames and runs through I3D \nnetwork concurrently, each frame is subjected to object \nidentification model, to generate person and object confidence \nscores. Tubelets are created by combining personal bounding \nboxes. Following, tubelets and object pieces (as nodes) are \nutilised to create an actor centric graph for each actor in the \nvideo [2]. \n3D convolution \n \nFig.6. 2D and 3D convolution [5]. \nBackbone for many of the state of the art models are 1-\nDimensional CNNs (C1D) uses 1D kernels, 2-Dimensional \nCNNs (C2D) uses 2D kernels, and 3-Dimensional CNNs \n(C3D) uses 3D kernels. C1D is generally used for \nProceedings of the Second International Conference on Electronics and Sustainable Communication Systems (ICESC-2021)\nIEEE Xplore Part Number: CFP21V66-ART; ISBN: 978-1-6654-2867-5\n978-1-6654-2867-5/21/$31.00 \u00a92021 IEEE 1186\nAuthorized licensed use limited to: b-on: Universidade de Lisboa Reitoria. Downloaded on May 06,2024 at 13:26:30 UTC from IEEE Xplore.  Restrictions apply. \nconvolutions of embedded type features along the time \ndimension, whilst C2D and C3D are used to extract the \nfeature vectors from single frames or layered frames. Figure 6 \nshows singular channel type samples of 2 Dimensional and 3 \nDimensional convolutions . Accurate hyperspectral image \nclassification has been a crucial yet difficult task. Convolution \nneural networks (CNNs) in two dimensions (2D) and three \ndimensions (3D) have been used to collect spectral or spatial \ninfo in multispectral photographs [5].  \nGraph is a form of structured arrangement of information that \nrepresents objects and their relationships. New research on \ngraph analysis has aroused a lot of interest using ML because \ngraphs have such a high expressive potential. In the graph \nfield, GNNs are based on deep learning algorithms. GNN has \nlately gained popularity as a graph monitoring system due to \nits superior performance. \n \nFig.7. Genetic model for GNN architecture [11]. \nFigure 7. Shows the genetic model of GNN architecture. \nGNN structures initial population is initialized to (S0) first, \nwith every individual being a multilayer Graph neural \nnetwork where every layer is made up of components chooses \nrandomly, such as the activation function, hidden embedding \nsize and aggregator. With respect to (S0) the GNN parameters \npopulation (P0) is then initialized and it sets parameters \naccordingly which evolves as the best suit (e.g., learning rate \nand dropout rate). Following that, to optimize the graph \nneural network structures using the optimal parameter setting \nfrom P0, architecture is made from S0 to S1 [11]. After the \nfirst round of alternate development between structure and \nparameter, it creates a GNN architecture with ideal design and \noptimum parameter settings produced from SI and PI. Six \nencoded states of GNN  architecture i.e., Hidden Dimension, \nAttention Head, Attention Function, Activation Function, \nAggregation Function, and Skip Connection.  \nFig.8. Design pipeline of GNN model [11]. \nFigure 8 shows the design pipeline for GNN model. \nPropagation Module is one of the most often utilized \ncomputational modules. Data is propagated across nodes \nusing propagation module, which allows data aggregation to \nrecord both topology and feature info. Convolution and \nrecurrent operators are typically employed in propagation \nmodules which gather the information from neighbours, skip \nconnection operation is typically employed to acquire detailed \ninfo from previous node representations. Sampling module \nfrequently require to carry out graph inclination. Sa mpling \nand propagation modules are frequently combined. To extract \nthe information, pooling modules are used from nodes when \nhigh-level subgraphs of graphs are needed [11] . A GNN \nmodel is usually developed by mixing these computing \ncomponents. The recurrent operator, convolutional operator, \nskip connection and sampling module are used to spread info \nin individual layer, and to retrieve high level information \npooling modules are added, as shown in figure 8. To get \nbetter representations, these layers are generally stacked. The \narchitecture used here can simplify GNN models and outliers, \nsuch as NDCN, which mixes GNNs and ordinary differential \nequation systems.  \nTransformers in video understanding - Action Transformers, \nin which 3D CNN characteristics are pooled and delivered to \nidentity exploit the Spatio-temporal data. By combining the \npatches generated frame at several time-steps, may use \ntransformer to classify video. GAT-GAT models, significance \nof a frame based on the local and worldwide circumstances \nusing an intra attention gate. This allows the network to \ncomprehend the video at multiple levels of granularity. \n \nIV. USAGE OF GNN  IN VARIOUS DOMAINS  \n \nGraph neural network practical applications include traffic \ncontrol [31] [32], human behavior detection [24] [25] , \n,adversarial attack prevention, recommender system, program \nverification, logical reasoning, molecular structure study, and \nsocial influence prediction Most GNN architectures can be \nclassed as structural and non-structural depending information \nthey process. Following are some intriguing applications from \neach categories : - In the graph-like structure of nano-scale \nmolecules, the nodes are ions and edges are bonds connecting \nthem according to GNN. In both cases, to learn about existing \nProceedings of the Second International Conference on Electronics and Sustainable Communication Systems (ICESC-2021)\nIEEE Xplore Part Number: CFP21V66-ART; ISBN: 978-1-6654-2867-5\n978-1-6654-2867-5/21/$31.00 \u00a92021 IEEE 1187\nAuthorized licensed use limited to: b-on: Universidade de Lisboa Reitoria. Downloaded on May 06,2024 at 13:26:30 UTC from IEEE Xplore.  Restrictions apply. \nmolecular structures and to discover unique chemical \nstructures GNNs can be used. This would have a great effect \non the development of computer assisted medication.  CNN \nare the most popular machine learning techniques which have \nremarkable answers to image categorization, a classic \ncomputer vision problem [20] [21] [22] [23] . GNNs can then \nbe used to enhance the ZSL picture classification recognition \ntask by applying them to this structured data. Text, like \nimages, does not have obvious relationships.  \n \nV. APPLICATIONS OF GNN  IN COMPUTER VISION  \n \nSome of the applications of GNN in computer vision \napplications are [26] [27] [ 28] [ 29] [30] [33] [34] ,   \n\uf0b7 Object Localization \n\uf0b7 Human-Object Interactions \n\uf0b7 Question Answering \n\uf0b7 Object Detection \n\uf0b7 Features Learning  \n\uf0b7 Image Classification \n\uf0b7 Relationships in a Photo \n\uf0b7 Visual Question Answering \n\uf0b7 Action Recognition \n\uf0b7 Point Clouds \n\uf0b7 3D Classification and Segmentation \n\uf0b7 RGBD Semantic Segmentation \n\uf0b7 Situation Recognition \n\uf0b7 Social Relationship Understanding \n\uf0b7 Zero-Shot Action Recognition \nGraph neural networks have the ability to immediately \nanalyze input graphs, thus incorporating its connectivity into \nthe product criteria. Most popular techniques to graph theory \nare based on a beginning stage that translates each graph over \nto a smaller data type, such as a vector or a series of reals. \nInteractions between humans and objects- GPNN repeatedly \nmodifies eigenvector matrices and node labeling within such \na passing messages inference framework. The V-COCO, \nHICO-DET and CAD-120 datasets are used for testing on 3 \nHOI identification benchmarks on images and videos. GPNN \nis adaptable to large datasets and can be used in both \nspatiotemporal scenarios. Visual QA is a graph-based way to \naddress visual questions. Object detection- proposes spatial-\ntemporal Graph Convolution Network (ST-GCN), a new \nmodel of dynamic skeleton that overcomes the constraints of \nearlier methods by understanding both spatial and temporal \nvariation from necessary data. Model based situation \nrecognition is a GNN that allows to record joint \ninterdependence between tasks effectively using neural \nnetwork models formed on a network. \n  \nVI. RESOURCES AND PLATFORMS FOR GRAPH \nCOMPUTING IN CV APPLICATIONS  \n  \nTABLE .I. STANDARD GRAPH LEARNI NG RESOURCES   \n \nTABLE II. PLATFORMS FOR GRAPH COMPUTING  \n \nTABLE III. GRAPH MODELS FOR COMPUTER VISION APPLICATIONS  \n \nTable 1 shows the repository and web links of standard graph \nlearning resources. Various platforms that can be used for \ngraph computing applications is tabulated in table 2. Table 3 \ndepicts the popular graph models can be used for computer \nvision applications.  \nVII. RESEARCH AND IMPLEMENTATION \nCHALLENGES  \nDespite the positive outcomes, previous works are continually \nhampered by the following two flaws: \nHyper parameters- Aside from GNN structure, a little change \nin hyper parameters can affect the performance of converging \nstructural model. Currently available approaches that simply \noptimize structural variables with fixed hyper parameter \nvalues may result in a model that is unsatisfactory. \nScalability - The time it takes to train recurrent network \ncontributes to the search time. Run-time computation would \nbe required for both the controller training and the single \nGNN model training. Furthermore, the controller often \nproduces and analyses potential GNN structures in a \nsequential fashion, which makes scaling to a vast searching \nspace problematic. \nVIII. CONCLUSION  \nDesign and implementation of Graph Neural Network (GNN) \nfor computer vision (CV) applications is currently an active \nongoing researching topic for various application domains not \nonly limited to CV. Still there are several unanswered \nconcerns. Spatio-temporal graph neural network architecture \nhas been described to explicitly simulate interactions between \nProceedings of the Second International Conference on Electronics and Sustainable Communication Systems (ICESC-2021)\nIEEE Xplore Part Number: CFP21V66-ART; ISBN: 978-1-6654-2867-5\n978-1-6654-2867-5/21/$31.00 \u00a92021 IEEE 1188\nAuthorized licensed use limited to: b-on: Universidade de Lisboa Reitoria. Downloaded on May 06,2024 at 13:26:30 UTC from IEEE Xplore.  Restrictions apply. \nactors, objects, and their environment. This approach can \nimplicitly or explicitly characterize objects, and it generalizes \nthe existing structured models for video comprehension. \nUsage of adaptive approach for better score, district task \nacross distinct dataset proposed. On AVA, still lot of \nadditional work needs to be done to better utilize explicit \nobject representations. \nFurther, Use of GNN in image and video understanding, \narchitecture, applications, resources, platforms, graph models \nand implementation challenges towards computer vision is \nelaborated in detail .  \nREFERENCES \n[1] Anurag Arnab et al., \u201cUnified Graph Structured Models for Video \nUnderstanding\u201d CVPR , arXiv:2103.15662 , 2021. \n[2] Yubo Zhang et al., \u201cA Structured Model For Action Detection\u201d CVPR,  \narXiv:1812.03544, 2019.   \n[3] Santiag o Castro et al.,\u201cFill -in-the-blank as a Challenging Video \nUnderstanding Evaluation Framework\u201d CVPR, arXiv:2104.04182 ,2021. \n[4] Saurabh Sahu et al.,\u201c  Enhancing  Transformer for Video Understanding  \nUsing Gated Multi -Level Attention and Temporal Adversarial  \nTraining\u201d CVPR, arXiv:2103.10043 , 2021.  \n[5] Matthew Hutchinson  et al.,\u201c  Video Action Understanding: A Tu torial\u201d \nCVPR, arXiv:2010.06647 , 2020.  \n[6] Ishan Dave  et al.,\u201cTCLR: Temporal Contrastive Learning for Video \nRepresentation\u201d CVPR, arXiv:2101.07974 , 2021.  \n[7] Gunnar A . Sigurdsson et al.,\u201c  Beyond the Camera: Neural Networks in \nWorld Coordinates\u201d CVPR,  arXiv:2003.05614 , 2020.  \n[8] Yubo Zhang et al.,\u201c  A Structured Model For Action Detection\u201d CVPR,  \narXiv:1812.03544,  2018.   \n[9] Hao Huang  et al.,\u201c  Dynamic Graph Modules for Modeling Object -\nObject Interactions in Activity Recognition \u201dCVPR, arXiv:1812.05637 , \n2018.  \n[10] Yunpeng  Chen et al.,\u201c  Graph -Based Global Reasoning  \nNetworks\u201d, CVPR,  arXiv:1811.12814 , 2018.  \n[11] Jie Zhou et al.,\u201cGraph Neural Networks: A Review of Methods and  \nApplications\u201d arXiv:1812.08434 , 2021 . \n[12] F. Scarselli  et al.,\u201c The Graph Neural Network Model,\u201d IEEE \nTransactions on Neural Networks,vol. 20, no. 1,  pp. 61 -80, 2009.  \n[13]  Karen Simonyan  et al.,\u201cTwo -Stream Convolu tional Networks  for \nAction Recognition in Videos\u201d Visual Geometry Group(VGG), \nUniversity of Oxford . \n[14] Rohit Girdhar et al.,\u201cVideo Action Transformer Network\u201d CVPR,  \narXiv:1812.02707, 2018.  \n[15] Siyuan Qi et al.,\u201c  Learning Hu man -Object Interactions by Graph \nParsing Neural Networks \u201d CVPR, arXiv:1808.07962 ,2018.  \n[16] Xin Guo et al.,\u201c  Graph Neural Networks for Image Understanding  \nBased on Multiple Cues: Grou p Emotion Recognition and Event \nRecognition as Use Cases\u201d CVPR, arXiv:1909.12911 , 2020.  \n[17] Danfei Xu et al.,\u201c  Scene Graph Generation b y Iterative Message \nPassing\u201d CVPR, arXiv:1701.02426 ,2017. \n[18] Yubo Zhang et al.,\u201c  A Structured Model For Action Detection\u201d CVPR,  \narXiv:1812.03544 , 2019.  \n[19] E. Mavroudi et al., \u201cRepresentation Learning on Visual -Symb olic \nGraphs for Video Understanding\u201d arXiv:1905.07385 , 2020.  \n[20] Biswas A et al.,\u201c  \u201cSurvey on Edge Computing \u2013Key Technology in \nRetail Industry\u201d Lecture Notes on Data Engineering and \nCommunications Technologies,  2021, vol 58. Springer, Singapore.  \n[21] R. J. Franklin  et al.,\u201c Anomaly Detection in Videos for Video \nSurveillance Applications using  Neural Networks,\u201d   Fourth \nInternational Conference on Inventive Systems and Control (ICISC) , \n2020 . \n[22] Mohana et al.,\u201cPerformance Evaluation of Background Modeling  \nMethods for  Object Detection and Tracking,\u201d 4th  International \nConference on Inventive Systems and Control (ICISC) , 2020.  \n[23] A. Biswas  et al.,\u201c Classification of Objects in Video Records using  \nNeural Network Framework,\u201d  International Conference on Smart \nSystems and Inventive Technology (ICSSIT) , 2018 . \n[24] H. Jain et al.,\u201c  Weapon Detection using Artificial I ntelligence and Deep \nLearning for Security Applications,\u201d  International Conference on \nElectronics and Sustainable Communication Systems (ICESC) , 2020 . [25] M. R. Nehashree  et al.,\u201c  Simulation and Performance Analysis of \nFeature Extraction and Matching Alg orithms for Image Processing  \nApplica tions,\u201d International Conference on Intelligent Sustainable \nSystems (ICISS),  2019.  \n[26] R. K. Meghana et al.,\u201c  Background -modelling  techniques for \nforeground detection and Tracking using G aussian Mixture Model,\u201d  3rd \nIntern ational Conference on Computing Methodologies and \nCommunicati on (ICCMC), 2019.  \n[27] V. P. Korakoppa  et al.,\u201c Implementation of highly efficient sorting  \nalgorithm for median filte ring using FPGA Spartan 6,\u201d  International \nConference on Innovative Mechanisms for In dustry Applications \n(ICIMIA), 2017 . \n[28] D. Akash et al.,\u201c Interfacing of flash memory and DDR3 RAM m emory \nwith Kintex 7 FPGA board,\u201d  International Conference on Recent \nTrends in Electronics, Information & Communication Technology \n(RTEICT) , 2017 . \n[29] N. Jain et al., \u201cPerformance Analysis of Object Detection and Tracking  \nAlgorithms for Traffic Surveillance Appl ications using Neural \nNetworks,\u201d  International conference on I -SMAC (IoT in Social, Mobile, \nAnalytics and Cloud) (I -SMAC) , 2019.  \n[30] C. Kumar B  et al.,\u201c YOLOv3 and YO LOv4: Multiple Object Detection  \nfor Surveillance Applications,\u201d  International Conference on Smart \nSystems and Inventive Technology (ICSSIT),  2020 . \n[31] C. Kumar B  et al.,\u201c  Performance Analysis of Object Detection \nAlgorithm for Intelligent Traffic Surveillance S ystem, \u201d  International \nConference on Inventive Research in Computing Applicatio ns \n(ICIRCA) , 2020.  \n[32] R. J. Franklin et al.,\u201c Traffic Signal Violation Detection using Artificial \nIntelligence and Deep Learning,\u201d  International Conference on \nCommunication and Ele ctronics Systems  (ICCES), 2020.  \n[33] Mohana  et al.,\u201c Object Detection and Tracking using Deep Learning  \nand Artificial Intelligence for Video Surveillance Applications\u201d \nInternational Journal of Advanced Computer Science and \nApplications(IJACSA), 10(12), 2019.    \n[34] Manoharan Samuel et al., \u201c Improved Version of  Graph -Cu t Algorithm \nfor CT Images of Lun g Cancer with Clinical Property Condition\u201d  \nJournal of Artificial Intelligence and C apsule networks,  2(4), 201 \u2013206. \n Proceedings of the Second International Conference on Electronics and Sustainable Communication Systems (ICESC-2021)\nIEEE Xplore Part Number: CFP21V66-ART; ISBN: 978-1-6654-2867-5\n978-1-6654-2867-5/21/$31.00 \u00a92021 IEEE 1189\nAuthorized licensed use limited to: b-on: Universidade de Lisboa Reitoria. Downloaded on May 06,2024 at 13:26:30 UTC from IEEE Xplore.  Restrictions apply."
    }
]